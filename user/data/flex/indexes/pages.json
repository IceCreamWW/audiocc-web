{"version":"1.5","timestamp":1712518110,"count":181,"index":{"":{"key":"","storage_key":"","template":null,"storage_timestamp":1704688029,"children":{"01.home":1704871816,"02.news":1704665586,"03.research":1704677623,"04.members":1704667918,"05.contact":1704688085,"backup":1704688029},"checksum":"2674c68c90233153549cd964092b2154"},"01.home":{"key":"home","storage_key":"01.home","template":"modular","storage_timestamp":1705239384,"markdown":{"en":{"modular":1705239384},"zh-hans":{"modular":1705239384}},"children":{"01._intro":1705239412,"02._all-stories":1704702097},"checksum":"f067072bc2322642524a926094f9f7e9"},"01.home\/01._intro":{"key":"home\/_intro","storage_key":"01.home\/01._intro","template":"intro","storage_timestamp":1705239412,"markdown":{"en":{"intro":1705239401},"zh-hans":{"intro":1705239274}},"checksum":"be0faf0b089ebb2a86edaed21f9eadbd"},"01.home\/02._all-stories":{"key":"home\/_all-stories","storage_key":"01.home\/02._all-stories","template":"allstories","storage_timestamp":1704702097,"markdown":{"en":{"allstories":1704688149},"zh-hans":{"allstories":1704688139}},"checksum":"cb62d0f6660ad8341393a59802c22a15"},"02.news":{"key":"news","storage_key":"02.news","template":"blog","storage_timestamp":1709540587,"markdown":{"en":{"blog":1704664726},"zh-hans":{"blog":1704665586}},"children":{"2024@\u5171\u542f\u65b0\u7bc7@@@\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u51ac\u5b63\u56e2\u5efa\u987a\u5229\u4e3e\u884c":0,"@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9762\u5411\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7684\u901a\u7528\u8de8\u8bed\u8a00\u6570\u636e\u589e\u5e7f":1709540642,"@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9ad8\u6548\u7684\u58f0\u7eb9\u8bc6\u522b\u6a21\u578b\u6784\u5efa\u65b9\u6cd5":1709540642,"\u9f99\u884c\u9f98\u9f98@\u524d\u7a0b\u6724\u6724@\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u65b0\u6625\u8d3a\u8bcd":1709540642},"checksum":"2614ee597ebddb1997513d2acb265561"},"02.news\/2024@\u5171\u542f\u65b0\u7bc7@@@\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u51ac\u5b63\u56e2\u5efa\u987a\u5229\u4e3e\u884c":{"key":"news\/2024@\u5171\u542f\u65b0\u7bc7@@@\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u51ac\u5b63\u56e2\u5efa\u987a\u5229\u4e3e\u884c","storage_key":"02.news\/2024@\u5171\u542f\u65b0\u7bc7@@@\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u51ac\u5b63\u56e2\u5efa\u987a\u5229\u4e3e\u884c","template":null,"storage_timestamp":0,"checksum":"28a425d35ebe1207fdf94b467d726e03"},"02.news\/@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9762\u5411\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7684\u901a\u7528\u8de8\u8bed\u8a00\u6570\u636e\u589e\u5e7f":{"key":"news\/@\u8bba\u6587\u901f\u9012@ieee@t-aslp\u8bba\u6587@\u9762\u5411\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7684\u901a\u7528\u8de8\u8bed\u8a00\u6570\u636e\u589e\u5e7f","storage_key":"02.news\/@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9762\u5411\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7684\u901a\u7528\u8de8\u8bed\u8a00\u6570\u636e\u589e\u5e7f","template":"item","storage_timestamp":1709540642,"markdown":{"en":{"item":1709540642},"zh-hans":{"item":1709540642}},"checksum":"73922c12db546007fcbbbc5dde61d529"},"02.news\/@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9ad8\u6548\u7684\u58f0\u7eb9\u8bc6\u522b\u6a21\u578b\u6784\u5efa\u65b9\u6cd5":{"key":"news\/@\u8bba\u6587\u901f\u9012@ieee@t-aslp\u8bba\u6587@\u9ad8\u6548\u7684\u58f0\u7eb9\u8bc6\u522b\u6a21\u578b\u6784\u5efa\u65b9\u6cd5","storage_key":"02.news\/@\u8bba\u6587\u901f\u9012@IEEE@T-ASLP\u8bba\u6587@\u9ad8\u6548\u7684\u58f0\u7eb9\u8bc6\u522b\u6a21\u578b\u6784\u5efa\u65b9\u6cd5","template":"item","storage_timestamp":1709540642,"markdown":{"en":{"item":1709540642},"zh-hans":{"item":1709540642}},"checksum":"343b5c613dec7c3618f3ec13fb4f7119"},"02.news\/\u9f99\u884c\u9f98\u9f98@\u524d\u7a0b\u6724\u6724@\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u65b0\u6625\u8d3a\u8bcd":{"key":"news\/\u9f99\u884c\u9f98\u9f98@\u524d\u7a0b\u6724\u6724@\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u65b0\u6625\u8d3a\u8bcd","storage_key":"02.news\/\u9f99\u884c\u9f98\u9f98@\u524d\u7a0b\u6724\u6724@\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\u542c\u89c9\u8ba4\u77e5\u4e0e\u8ba1\u7b97\u58f0\u5b66\u5b9e\u9a8c\u5ba4\u65b0\u6625\u8d3a\u8bcd","template":"item","storage_timestamp":1709540642,"markdown":{"en":{"item":1709540642},"zh-hans":{"item":1709540642}},"checksum":"92b29a97a8dfe08280079b5c5ecf8c23"},"03.research":{"key":"research","storage_key":"03.research","template":"default","storage_timestamp":1712514177,"markdown":{"en":{"default":1704667619},"zh-hans":{"default":1704667607}},"children":{"01.fields":1712493471,"02.publications":1712511400,"bibtex_helper":1712515162,"_03.achievements":1705239539},"checksum":"0cd551f656470a56d24a3559a36cb06c"},"03.research\/01.fields":{"key":"research\/fields","storage_key":"03.research\/01.fields","template":"default","storage_timestamp":1712493471,"markdown":{"":{"default":1709606280},"zh-hans":{"blog":1709601890}},"children":{"\u8bed\u97f3\u8bc6\u522b":1712493554},"checksum":"b88cd0d773065a012c5fba61055de7e6"},"03.research\/01.fields\/\u8bed\u97f3\u8bc6\u522b":{"key":"research\/fields\/\u8bed\u97f3\u8bc6\u522b","storage_key":"03.research\/01.fields\/\u8bed\u97f3\u8bc6\u522b","template":"item","storage_timestamp":1712493554,"markdown":{"en":{"item":1712493554},"zh-hans":{"item":1712493534}},"checksum":"bee52e3fe567e3735bfaea311ce2a86c"},"03.research\/02.publications":{"key":"research\/publications","storage_key":"03.research\/02.publications","template":"publications","storage_timestamp":1712511400,"markdown":{"en":{"publications":1709120034},"zh-hans":{"publications":1709606280}},"children":{"2019":1712511399,"2020":1712511399,"2021":1712511399,"2022":1712511400,"2023":1712511400,"2024":1712511400},"checksum":"5e83c8de066c182d1c82ae74855ebe7d"},"03.research\/02.publications\/2019":{"key":"research\/publications\/2019","storage_key":"03.research\/02.publications\/2019","template":null,"storage_timestamp":1712511399,"children":{"@Cross-Domain@Replay@Spoofing@Attack@Detection@Using@Domain@Adversarial@Training@":1712513560,"@Data@Augmentation@Using@Variational@Autoencoder@for@Embedding@Based@Speaker@Verification@":1712511399,"@Joint@Decoding@of@CTC@Based@Systems@for@Speech@Recognition@":1712511399,"@Knowledge@Distillation@for@End-to-End@Monaural@Multi-Talker@ASR@System@":1712511399,"@On@the@Usage@of@Phonetic@Information@for@Text-Independent@Speaker@Embedding@Extraction@":1712511399,"@Prosody@Usage@Optimization@for@Children@Speech@Recognition@with@Zero@Resource@Children@Speech@":1712511399,"@Robust@DOA@Estimation@Based@on@Convolutional@Neural@Network@and@Time-Frequency@Masking@":1712511399,"@The@SJTU@Robust@Anti-Spoofing@System@for@the@ASVspoof@2019@Challenge@":1712511399,"Discriminative@Neural@Embedding@Learning@for@Short-Duration@Text-Independent@Speaker@Verification":1712511399,"End-to-end@Monaural@Multi-speaker@ASR@System@without@Pretraining":1712511399,"End-to-End@Overlapped@Speech@Detection@and@Speaker@Counting@with@Raw@Waveform":1712511399,"Exploring@Model@Units@and@Training@Strategies@for@End-to-End@Speech@Recognition":1712511399,"GANs@for@Children@@A@Generative@Data@Augmentation@Strategy@for@Children@Speech@Recognition":0,"Knowledge@Distillation@for@Small@Foot-print@Deep@Speaker@Embedding":1712511399,"Margin@Matters@@Towards@More@Discriminative@Deep@Neural@Network@Embeddings@for@Speaker@Recognition":0,"MIMO-Speech@@End-to-End@Multi-Channel@Multi-Speaker@Speech@Recognition":0},"checksum":"2afbac5d674c3ea9e15161c4a54b6f78"},"03.research\/02.publications\/2019\/@Cross-Domain@Replay@Spoofing@Attack@Detection@Using@Domain@Adversarial@Training@":{"key":"research\/publications\/2019\/@cross-domain@replay@spoofing@attack@detection@using@domain@adversarial@training@","storage_key":"03.research\/02.publications\/2019\/@Cross-Domain@Replay@Spoofing@Attack@Detection@Using@Domain@Adversarial@Training@","template":"default","storage_timestamp":1712513560,"markdown":{"":{"default":1712511399}},"checksum":"b4f9ab32f3cfcf2bc3151a2367183f75"},"03.research\/02.publications\/2019\/@Data@Augmentation@Using@Variational@Autoencoder@for@Embedding@Based@Speaker@Verification@":{"key":"research\/publications\/2019\/@data@augmentation@using@variational@autoencoder@for@embedding@based@speaker@verification@","storage_key":"03.research\/02.publications\/2019\/@Data@Augmentation@Using@Variational@Autoencoder@for@Embedding@Based@Speaker@Verification@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"c90279db912bd6d03311361c306101f6"},"03.research\/02.publications\/2019\/@Joint@Decoding@of@CTC@Based@Systems@for@Speech@Recognition@":{"key":"research\/publications\/2019\/@joint@decoding@of@ctc@based@systems@for@speech@recognition@","storage_key":"03.research\/02.publications\/2019\/@Joint@Decoding@of@CTC@Based@Systems@for@Speech@Recognition@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"07b7bc67b95e47395be3770456b6a0e2"},"03.research\/02.publications\/2019\/@Knowledge@Distillation@for@End-to-End@Monaural@Multi-Talker@ASR@System@":{"key":"research\/publications\/2019\/@knowledge@distillation@for@end-to-end@monaural@multi-talker@asr@system@","storage_key":"03.research\/02.publications\/2019\/@Knowledge@Distillation@for@End-to-End@Monaural@Multi-Talker@ASR@System@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"37a83d59e1d79ba9329a902638375787"},"03.research\/02.publications\/2019\/@On@the@Usage@of@Phonetic@Information@for@Text-Independent@Speaker@Embedding@Extraction@":{"key":"research\/publications\/2019\/@on@the@usage@of@phonetic@information@for@text-independent@speaker@embedding@extraction@","storage_key":"03.research\/02.publications\/2019\/@On@the@Usage@of@Phonetic@Information@for@Text-Independent@Speaker@Embedding@Extraction@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"7ea4e58a3c54ae030caee7bd6f45ee63"},"03.research\/02.publications\/2019\/@Prosody@Usage@Optimization@for@Children@Speech@Recognition@with@Zero@Resource@Children@Speech@":{"key":"research\/publications\/2019\/@prosody@usage@optimization@for@children@speech@recognition@with@zero@resource@children@speech@","storage_key":"03.research\/02.publications\/2019\/@Prosody@Usage@Optimization@for@Children@Speech@Recognition@with@Zero@Resource@Children@Speech@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"e836b2dacf54781fc6a8959f1f750b03"},"03.research\/02.publications\/2019\/@Robust@DOA@Estimation@Based@on@Convolutional@Neural@Network@and@Time-Frequency@Masking@":{"key":"research\/publications\/2019\/@robust@doa@estimation@based@on@convolutional@neural@network@and@time-frequency@masking@","storage_key":"03.research\/02.publications\/2019\/@Robust@DOA@Estimation@Based@on@Convolutional@Neural@Network@and@Time-Frequency@Masking@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"131aeec4abec8c96136584c52a983985"},"03.research\/02.publications\/2019\/@The@SJTU@Robust@Anti-Spoofing@System@for@the@ASVspoof@2019@Challenge@":{"key":"research\/publications\/2019\/@the@sjtu@robust@anti-spoofing@system@for@the@asvspoof@2019@challenge@","storage_key":"03.research\/02.publications\/2019\/@The@SJTU@Robust@Anti-Spoofing@System@for@the@ASVspoof@2019@Challenge@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"55d08f17a9fe9720004712e6f706ca41"},"03.research\/02.publications\/2019\/Discriminative@Neural@Embedding@Learning@for@Short-Duration@Text-Independent@Speaker@Verification":{"key":"research\/publications\/2019\/discriminative@neural@embedding@learning@for@short-duration@text-independent@speaker@verification","storage_key":"03.research\/02.publications\/2019\/Discriminative@Neural@Embedding@Learning@for@Short-Duration@Text-Independent@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"8323c645ddb8cae660f9273aa3d6a366"},"03.research\/02.publications\/2019\/End-to-end@Monaural@Multi-speaker@ASR@System@without@Pretraining":{"key":"research\/publications\/2019\/end-to-end@monaural@multi-speaker@asr@system@without@pretraining","storage_key":"03.research\/02.publications\/2019\/End-to-end@Monaural@Multi-speaker@ASR@System@without@Pretraining","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"00c85decca4dd504f8cd04eb7b21adc7"},"03.research\/02.publications\/2019\/End-to-End@Overlapped@Speech@Detection@and@Speaker@Counting@with@Raw@Waveform":{"key":"research\/publications\/2019\/end-to-end@overlapped@speech@detection@and@speaker@counting@with@raw@waveform","storage_key":"03.research\/02.publications\/2019\/End-to-End@Overlapped@Speech@Detection@and@Speaker@Counting@with@Raw@Waveform","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"42e78a04a23160f2a156efc89677f030"},"03.research\/02.publications\/2019\/Exploring@Model@Units@and@Training@Strategies@for@End-to-End@Speech@Recognition":{"key":"research\/publications\/2019\/exploring@model@units@and@training@strategies@for@end-to-end@speech@recognition","storage_key":"03.research\/02.publications\/2019\/Exploring@Model@Units@and@Training@Strategies@for@End-to-End@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"c66702163873bdf9561248e33644836e"},"03.research\/02.publications\/2019\/GANs@for@Children@@A@Generative@Data@Augmentation@Strategy@for@Children@Speech@Recognition":{"key":"research\/publications\/2019\/gans@for@children@@a@generative@data@augmentation@strategy@for@children@speech@recognition","storage_key":"03.research\/02.publications\/2019\/GANs@for@Children@@A@Generative@Data@Augmentation@Strategy@for@Children@Speech@Recognition","template":null,"storage_timestamp":0,"checksum":"d3294be2ac56fd7323461d33dbdefe7c"},"03.research\/02.publications\/2019\/Knowledge@Distillation@for@Small@Foot-print@Deep@Speaker@Embedding":{"key":"research\/publications\/2019\/knowledge@distillation@for@small@foot-print@deep@speaker@embedding","storage_key":"03.research\/02.publications\/2019\/Knowledge@Distillation@for@Small@Foot-print@Deep@Speaker@Embedding","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"d619a5b685b374b4fbc5835dd13164e0"},"03.research\/02.publications\/2019\/Margin@Matters@@Towards@More@Discriminative@Deep@Neural@Network@Embeddings@for@Speaker@Recognition":{"key":"research\/publications\/2019\/margin@matters@@towards@more@discriminative@deep@neural@network@embeddings@for@speaker@recognition","storage_key":"03.research\/02.publications\/2019\/Margin@Matters@@Towards@More@Discriminative@Deep@Neural@Network@Embeddings@for@Speaker@Recognition","template":null,"storage_timestamp":0,"checksum":"2e4403a17c5088792b2baa417a574de3"},"03.research\/02.publications\/2019\/MIMO-Speech@@End-to-End@Multi-Channel@Multi-Speaker@Speech@Recognition":{"key":"research\/publications\/2019\/mimo-speech@@end-to-end@multi-channel@multi-speaker@speech@recognition","storage_key":"03.research\/02.publications\/2019\/MIMO-Speech@@End-to-End@Multi-Channel@Multi-Speaker@Speech@Recognition","template":null,"storage_timestamp":0,"checksum":"e88d8aec5ac4812d003115158f32a694"},"03.research\/02.publications\/2020":{"key":"research\/publications\/2020","storage_key":"03.research\/02.publications\/2020","template":null,"storage_timestamp":1712511399,"children":{"@Adversarial@Domain@Adaptation@for@Speaker@Verification@Using@Partially@Shared@Network@":1712511399,"@Bi-Encoder@Transformer@Network@for@Mandarin-English@Code-Switching@Speech@Recognition@Using@Mixture@of@Experts@":1712511399,"@Dual-Adversarial@Domain@Adaptation@for@Generalized@Replay@Attack@Detection@":1712511399,"@End-to-End@Far-Field@Speech@Recognition@with@Unified@Dereverberation@and@Beamforming@":1712511399,"@Learning@Contextual@Language@Embeddings@for@Monaural@Multi-Talker@Speech@Recognition@":1712511399,"@Listen@@Watch@and@Understand@at@the@Cocktail@Party@@Audio-Visual-Contextual@Speech@Separation@":0,"@Multi-Modality@Matters@@A@Performance@Leap@on@VoxCeleb@":0,"Channel@Invariant@Speaker@Embedding@Learning@with@Joint@Multi-Task@and@Adversarial@Training":1712511399,"Data@Augmentation@Using@Deep@Generative@Models@for@Embedding@Based@Speaker@Recognition":1712511399,"Deep@Audio-Visual@Speech@Separation@with@Attention@Mechanism":1712511399,"End-To-End@Multi-Speaker@Speech@Recognition@With@Transformer":1712511399,"Improving@End-to-End@Single-Channel@Multi-Talker@Speech@Recognition":1712511399,"Text@Adaptation@for@Speaker@Verification@with@Speaker-Text@Factorized@Embeddings":1712511399},"checksum":"3edb02191f796a5c211fb0fbe5c60034"},"03.research\/02.publications\/2020\/@Adversarial@Domain@Adaptation@for@Speaker@Verification@Using@Partially@Shared@Network@":{"key":"research\/publications\/2020\/@adversarial@domain@adaptation@for@speaker@verification@using@partially@shared@network@","storage_key":"03.research\/02.publications\/2020\/@Adversarial@Domain@Adaptation@for@Speaker@Verification@Using@Partially@Shared@Network@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"ff6a891ae396421ead1039e81fe7eadd"},"03.research\/02.publications\/2020\/@Bi-Encoder@Transformer@Network@for@Mandarin-English@Code-Switching@Speech@Recognition@Using@Mixture@of@Experts@":{"key":"research\/publications\/2020\/@bi-encoder@transformer@network@for@mandarin-english@code-switching@speech@recognition@using@mixture@of@experts@","storage_key":"03.research\/02.publications\/2020\/@Bi-Encoder@Transformer@Network@for@Mandarin-English@Code-Switching@Speech@Recognition@Using@Mixture@of@Experts@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"593b4215798dbe50e3500f8f0e2c99cd"},"03.research\/02.publications\/2020\/@Dual-Adversarial@Domain@Adaptation@for@Generalized@Replay@Attack@Detection@":{"key":"research\/publications\/2020\/@dual-adversarial@domain@adaptation@for@generalized@replay@attack@detection@","storage_key":"03.research\/02.publications\/2020\/@Dual-Adversarial@Domain@Adaptation@for@Generalized@Replay@Attack@Detection@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"a50461c9eba7c8a738b9cd3f92575ee8"},"03.research\/02.publications\/2020\/@End-to-End@Far-Field@Speech@Recognition@with@Unified@Dereverberation@and@Beamforming@":{"key":"research\/publications\/2020\/@end-to-end@far-field@speech@recognition@with@unified@dereverberation@and@beamforming@","storage_key":"03.research\/02.publications\/2020\/@End-to-End@Far-Field@Speech@Recognition@with@Unified@Dereverberation@and@Beamforming@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"1bd644bba25b04fc963ebf53000427ba"},"03.research\/02.publications\/2020\/@Learning@Contextual@Language@Embeddings@for@Monaural@Multi-Talker@Speech@Recognition@":{"key":"research\/publications\/2020\/@learning@contextual@language@embeddings@for@monaural@multi-talker@speech@recognition@","storage_key":"03.research\/02.publications\/2020\/@Learning@Contextual@Language@Embeddings@for@Monaural@Multi-Talker@Speech@Recognition@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"c4b1716176268ce0c0785e333ef65b0b"},"03.research\/02.publications\/2020\/@Listen@@Watch@and@Understand@at@the@Cocktail@Party@@Audio-Visual-Contextual@Speech@Separation@":{"key":"research\/publications\/2020\/@listen@@watch@and@understand@at@the@cocktail@party@@audio-visual-contextual@speech@separation@","storage_key":"03.research\/02.publications\/2020\/@Listen@@Watch@and@Understand@at@the@Cocktail@Party@@Audio-Visual-Contextual@Speech@Separation@","template":null,"storage_timestamp":0,"checksum":"aaff074942094e4da26769cad75900da"},"03.research\/02.publications\/2020\/@Multi-Modality@Matters@@A@Performance@Leap@on@VoxCeleb@":{"key":"research\/publications\/2020\/@multi-modality@matters@@a@performance@leap@on@voxceleb@","storage_key":"03.research\/02.publications\/2020\/@Multi-Modality@Matters@@A@Performance@Leap@on@VoxCeleb@","template":null,"storage_timestamp":0,"checksum":"366abf96dc9d20174b88982ab13efc65"},"03.research\/02.publications\/2020\/Channel@Invariant@Speaker@Embedding@Learning@with@Joint@Multi-Task@and@Adversarial@Training":{"key":"research\/publications\/2020\/channel@invariant@speaker@embedding@learning@with@joint@multi-task@and@adversarial@training","storage_key":"03.research\/02.publications\/2020\/Channel@Invariant@Speaker@Embedding@Learning@with@Joint@Multi-Task@and@Adversarial@Training","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"477c434ec8558160f25e7e719e703a1b"},"03.research\/02.publications\/2020\/Data@Augmentation@Using@Deep@Generative@Models@for@Embedding@Based@Speaker@Recognition":{"key":"research\/publications\/2020\/data@augmentation@using@deep@generative@models@for@embedding@based@speaker@recognition","storage_key":"03.research\/02.publications\/2020\/Data@Augmentation@Using@Deep@Generative@Models@for@Embedding@Based@Speaker@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"0da7be8ba22745b07a4b8d427bee7ae2"},"03.research\/02.publications\/2020\/Deep@Audio-Visual@Speech@Separation@with@Attention@Mechanism":{"key":"research\/publications\/2020\/deep@audio-visual@speech@separation@with@attention@mechanism","storage_key":"03.research\/02.publications\/2020\/Deep@Audio-Visual@Speech@Separation@with@Attention@Mechanism","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"36ef29336bacdc39a5f71435443a68d8"},"03.research\/02.publications\/2020\/End-To-End@Multi-Speaker@Speech@Recognition@With@Transformer":{"key":"research\/publications\/2020\/end-to-end@multi-speaker@speech@recognition@with@transformer","storage_key":"03.research\/02.publications\/2020\/End-To-End@Multi-Speaker@Speech@Recognition@With@Transformer","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"a1a07d5614ce140073db218c2f4dc1c8"},"03.research\/02.publications\/2020\/Improving@End-to-End@Single-Channel@Multi-Talker@Speech@Recognition":{"key":"research\/publications\/2020\/improving@end-to-end@single-channel@multi-talker@speech@recognition","storage_key":"03.research\/02.publications\/2020\/Improving@End-to-End@Single-Channel@Multi-Talker@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"456285c48c3722105d99c2b721aaeb54"},"03.research\/02.publications\/2020\/Text@Adaptation@for@Speaker@Verification@with@Speaker-Text@Factorized@Embeddings":{"key":"research\/publications\/2020\/text@adaptation@for@speaker@verification@with@speaker-text@factorized@embeddings","storage_key":"03.research\/02.publications\/2020\/Text@Adaptation@for@Speaker@Verification@with@Speaker-Text@Factorized@Embeddings","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"b0bd134b8a94b1d6c2bffd08d71cc0f2"},"03.research\/02.publications\/2021":{"key":"research\/publications\/2021","storage_key":"03.research\/02.publications\/2021","template":null,"storage_timestamp":1712511399,"children":{"@Audio-Visual@Multi-Talker@Speech@Recognition@in@a@Cocktail@Party@":1712511399,"@Basis-MelGAN@@Efficient@Neural@Vocoder@Based@on@Audio@Decomposition@":0,"@Knowledge@Distillation@from@Multi-Modality@to@Single-Modality@for@Person@Verification@":1712511399,"@Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition@":1712511399,"@The@SJTU@System@for@Short-Duration@Speaker@Verification@Challenge@2021@":1712511399,"AISpeech-SJTU@Accent@Identification@System@for@the@Accented@English@Speech@Recognition@Challenge":1712511399,"AISpeech-SJTU@ASR@System@for@the@Accented@English@Speech@Recognition@Challenge":1712511399,"Audio-Visual@Deep@Neural@Network@for@Robust@Person@Verification":1712511399,"Closing@the@Gap@Between@Time-Domain@Multi-Channel@Speech@Enhancement@on@Real@and@Simulation@Conditions":1712511399,"Convolutive@Transfer@Function@Invariant@SDR@Training@Criteria@for@Multi-Channel@Reverberant@Speech@Separation":1712511399,"Data@Augmentation@for@end-to-end@Code-Switching@Speech@Recognition":1712511399,"Dual-Path@Modeling@for@Long@Recording@Speech@Separation@in@Meetings":1712511399,"Dual-Path@RNN@for@Long@Recording@Speech@Separation":1712511399,"End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@with@Improved@Numerical@Stability@and@Advanced@Frontend":0,"Modified@Magnitude-Phase@Spectrum@Information@for@Spoofing@Detection":1712511399,"Revisiting@the@Statistics@Pooling@Layer@in@Deep@Speaker@Embedding@Learning":1712511399,"Self-Supervised@Learning@Based@Domain@Adaptation@for@Robust@Speaker@Verification":1712511399,"Speaker@Embedding@Augmentation@with@Noise@Distribution@Matching":1712511399,"SynAug@@Synthesis-Based@Data@Augmentation@for@Text-Dependent@Speaker@Verification":0,"The@Accented@English@Speech@Recognition@Challenge@2020@@Open@Datasets@@Tracks@@Baselines@@Results@and@Methods":0,"Towards@Data@Selection@on@TTS@Data@for@Children@s@Speech@Recognition":1712511399,"Unit@Selection@Synthesis@Based@Data@Augmentation@for@Fixed@Phrase@Speaker@Verification":1712511399},"checksum":"cdd802eb24ebabdf4f9cb04b68d9f85d"},"03.research\/02.publications\/2021\/@Audio-Visual@Multi-Talker@Speech@Recognition@in@a@Cocktail@Party@":{"key":"research\/publications\/2021\/@audio-visual@multi-talker@speech@recognition@in@a@cocktail@party@","storage_key":"03.research\/02.publications\/2021\/@Audio-Visual@Multi-Talker@Speech@Recognition@in@a@Cocktail@Party@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"06bc3ecd473441b56bf6d76161a7e5da"},"03.research\/02.publications\/2021\/@Basis-MelGAN@@Efficient@Neural@Vocoder@Based@on@Audio@Decomposition@":{"key":"research\/publications\/2021\/@basis-melgan@@efficient@neural@vocoder@based@on@audio@decomposition@","storage_key":"03.research\/02.publications\/2021\/@Basis-MelGAN@@Efficient@Neural@Vocoder@Based@on@Audio@Decomposition@","template":null,"storage_timestamp":0,"checksum":"09c95257d7e90beb2460e27f479d5983"},"03.research\/02.publications\/2021\/@Knowledge@Distillation@from@Multi-Modality@to@Single-Modality@for@Person@Verification@":{"key":"research\/publications\/2021\/@knowledge@distillation@from@multi-modality@to@single-modality@for@person@verification@","storage_key":"03.research\/02.publications\/2021\/@Knowledge@Distillation@from@Multi-Modality@to@Single-Modality@for@Person@Verification@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"65a0beeb515505e69b3fd85a4fa18cd5"},"03.research\/02.publications\/2021\/@Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition@":{"key":"research\/publications\/2021\/@layer-wise@fast@adaptation@for@end-to-end@multi-accent@speech@recognition@","storage_key":"03.research\/02.publications\/2021\/@Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"a0f2356b51e60bb674d948a7d684c6be"},"03.research\/02.publications\/2021\/@The@SJTU@System@for@Short-Duration@Speaker@Verification@Challenge@2021@":{"key":"research\/publications\/2021\/@the@sjtu@system@for@short-duration@speaker@verification@challenge@2021@","storage_key":"03.research\/02.publications\/2021\/@The@SJTU@System@for@Short-Duration@Speaker@Verification@Challenge@2021@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"65d59e1356972b542960c3602e49bc3b"},"03.research\/02.publications\/2021\/AISpeech-SJTU@Accent@Identification@System@for@the@Accented@English@Speech@Recognition@Challenge":{"key":"research\/publications\/2021\/aispeech-sjtu@accent@identification@system@for@the@accented@english@speech@recognition@challenge","storage_key":"03.research\/02.publications\/2021\/AISpeech-SJTU@Accent@Identification@System@for@the@Accented@English@Speech@Recognition@Challenge","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"25dbe2650a26aa8e122e0dc449c93eb3"},"03.research\/02.publications\/2021\/AISpeech-SJTU@ASR@System@for@the@Accented@English@Speech@Recognition@Challenge":{"key":"research\/publications\/2021\/aispeech-sjtu@asr@system@for@the@accented@english@speech@recognition@challenge","storage_key":"03.research\/02.publications\/2021\/AISpeech-SJTU@ASR@System@for@the@Accented@English@Speech@Recognition@Challenge","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"dfa9ce00972132afb1a3250fc8771006"},"03.research\/02.publications\/2021\/Audio-Visual@Deep@Neural@Network@for@Robust@Person@Verification":{"key":"research\/publications\/2021\/audio-visual@deep@neural@network@for@robust@person@verification","storage_key":"03.research\/02.publications\/2021\/Audio-Visual@Deep@Neural@Network@for@Robust@Person@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"d71d7a8ba95c19824a70136bc4daaf5b"},"03.research\/02.publications\/2021\/Closing@the@Gap@Between@Time-Domain@Multi-Channel@Speech@Enhancement@on@Real@and@Simulation@Conditions":{"key":"research\/publications\/2021\/closing@the@gap@between@time-domain@multi-channel@speech@enhancement@on@real@and@simulation@conditions","storage_key":"03.research\/02.publications\/2021\/Closing@the@Gap@Between@Time-Domain@Multi-Channel@Speech@Enhancement@on@Real@and@Simulation@Conditions","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"31f7d61b23a60ae96b5c46a01c7e7edc"},"03.research\/02.publications\/2021\/Convolutive@Transfer@Function@Invariant@SDR@Training@Criteria@for@Multi-Channel@Reverberant@Speech@Separation":{"key":"research\/publications\/2021\/convolutive@transfer@function@invariant@sdr@training@criteria@for@multi-channel@reverberant@speech@separation","storage_key":"03.research\/02.publications\/2021\/Convolutive@Transfer@Function@Invariant@SDR@Training@Criteria@for@Multi-Channel@Reverberant@Speech@Separation","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"11962bd37c5c0375a92a93d1ed0b3d70"},"03.research\/02.publications\/2021\/Data@Augmentation@for@end-to-end@Code-Switching@Speech@Recognition":{"key":"research\/publications\/2021\/data@augmentation@for@end-to-end@code-switching@speech@recognition","storage_key":"03.research\/02.publications\/2021\/Data@Augmentation@for@end-to-end@Code-Switching@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"bd51a296f5b23b8a0b6d9522a4409f7a"},"03.research\/02.publications\/2021\/Dual-Path@Modeling@for@Long@Recording@Speech@Separation@in@Meetings":{"key":"research\/publications\/2021\/dual-path@modeling@for@long@recording@speech@separation@in@meetings","storage_key":"03.research\/02.publications\/2021\/Dual-Path@Modeling@for@Long@Recording@Speech@Separation@in@Meetings","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"3bd6e3e16b8a119a6615d2f589a04d3f"},"03.research\/02.publications\/2021\/Dual-Path@RNN@for@Long@Recording@Speech@Separation":{"key":"research\/publications\/2021\/dual-path@rnn@for@long@recording@speech@separation","storage_key":"03.research\/02.publications\/2021\/Dual-Path@RNN@for@Long@Recording@Speech@Separation","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"d42b0e7ba628adc773f74627f1e16608"},"03.research\/02.publications\/2021\/End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@with@Improved@Numerical@Stability@and@Advanced@Frontend":{"key":"research\/publications\/2021\/end-to-end@dereverberation@@beamforming@@and@speech@recognition@with@improved@numerical@stability@and@advanced@frontend","storage_key":"03.research\/02.publications\/2021\/End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@with@Improved@Numerical@Stability@and@Advanced@Frontend","template":null,"storage_timestamp":0,"checksum":"6d3b79173244aa1a4588d52f311b5a21"},"03.research\/02.publications\/2021\/Modified@Magnitude-Phase@Spectrum@Information@for@Spoofing@Detection":{"key":"research\/publications\/2021\/modified@magnitude-phase@spectrum@information@for@spoofing@detection","storage_key":"03.research\/02.publications\/2021\/Modified@Magnitude-Phase@Spectrum@Information@for@Spoofing@Detection","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"650606e37e1bab05528c312bf78bc4a3"},"03.research\/02.publications\/2021\/Revisiting@the@Statistics@Pooling@Layer@in@Deep@Speaker@Embedding@Learning":{"key":"research\/publications\/2021\/revisiting@the@statistics@pooling@layer@in@deep@speaker@embedding@learning","storage_key":"03.research\/02.publications\/2021\/Revisiting@the@Statistics@Pooling@Layer@in@Deep@Speaker@Embedding@Learning","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"638d29316de23a3dbca21f28367d0c3a"},"03.research\/02.publications\/2021\/Self-Supervised@Learning@Based@Domain@Adaptation@for@Robust@Speaker@Verification":{"key":"research\/publications\/2021\/self-supervised@learning@based@domain@adaptation@for@robust@speaker@verification","storage_key":"03.research\/02.publications\/2021\/Self-Supervised@Learning@Based@Domain@Adaptation@for@Robust@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"3a24122b6c21d2fed139da159bbfc26a"},"03.research\/02.publications\/2021\/Speaker@Embedding@Augmentation@with@Noise@Distribution@Matching":{"key":"research\/publications\/2021\/speaker@embedding@augmentation@with@noise@distribution@matching","storage_key":"03.research\/02.publications\/2021\/Speaker@Embedding@Augmentation@with@Noise@Distribution@Matching","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"6d50c9f8d48ad14495fdb57801117e25"},"03.research\/02.publications\/2021\/SynAug@@Synthesis-Based@Data@Augmentation@for@Text-Dependent@Speaker@Verification":{"key":"research\/publications\/2021\/synaug@@synthesis-based@data@augmentation@for@text-dependent@speaker@verification","storage_key":"03.research\/02.publications\/2021\/SynAug@@Synthesis-Based@Data@Augmentation@for@Text-Dependent@Speaker@Verification","template":null,"storage_timestamp":0,"checksum":"f5bda40432084ce4e5c7bd52216cc460"},"03.research\/02.publications\/2021\/The@Accented@English@Speech@Recognition@Challenge@2020@@Open@Datasets@@Tracks@@Baselines@@Results@and@Methods":{"key":"research\/publications\/2021\/the@accented@english@speech@recognition@challenge@2020@@open@datasets@@tracks@@baselines@@results@and@methods","storage_key":"03.research\/02.publications\/2021\/The@Accented@English@Speech@Recognition@Challenge@2020@@Open@Datasets@@Tracks@@Baselines@@Results@and@Methods","template":null,"storage_timestamp":0,"checksum":"9132c68f5a207e2dbbf7e7533effde31"},"03.research\/02.publications\/2021\/Towards@Data@Selection@on@TTS@Data@for@Children@s@Speech@Recognition":{"key":"research\/publications\/2021\/towards@data@selection@on@tts@data@for@children@s@speech@recognition","storage_key":"03.research\/02.publications\/2021\/Towards@Data@Selection@on@TTS@Data@for@Children@s@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"3615f86245a6656dd2da47f859ec1510"},"03.research\/02.publications\/2021\/Unit@Selection@Synthesis@Based@Data@Augmentation@for@Fixed@Phrase@Speaker@Verification":{"key":"research\/publications\/2021\/unit@selection@synthesis@based@data@augmentation@for@fixed@phrase@speaker@verification","storage_key":"03.research\/02.publications\/2021\/Unit@Selection@Synthesis@Based@Data@Augmentation@for@Fixed@Phrase@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"32c8702b13b557a513b788740f8d9fd7"},"03.research\/02.publications\/2022":{"key":"research\/publications\/2022","storage_key":"03.research\/02.publications\/2022","template":null,"storage_timestamp":1712511400,"children":{"@Attentive@Feature@Fusion@for@Robust@Speaker@Verification@":1712511399,"@DF-ResNet@@Boosting@Speaker@Verification@Performance@with@Depth-First@Design@":0,"@Dual@Path@Embedding@Learning@for@Speaker@Verification@with@Triplet@Attention@":1712511400,"@Enroll-Aware@Attentive@Statistics@Pooling@for@Target@Speaker@Verification@":1712511399,"@ESPnet-SE++@@Speech@Enhancement@for@Robust@Speech@Recognition@@Translation@@and@Understanding@":0,"@Knowledge@Transfer@and@Distillation@from@Autoregressive@to@Non-Autoregessive@Speech@Recognition@":1712511399,"@MSDWild@@Multi-modal@Speaker@Diarization@Dataset@in@the@Wild@":0,"@Self-Supervised@Speaker@Verification@Using@Dynamic@Loss-Gate@and@Label@Correction@":1712511399,"@Separating@Long-Form@Speech@with@Group-wise@Permutation@Invariant@Training@":1712511399,"Dual-Path@Modeling@With@Memory@Embedding@Model@for@Continuous@Speech@Separation":1712511399,"End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@in@a@Cocktail@Party":0,"Exploring@Effective@Data@Utilization@for@Low-Resource@Speech@Recognition":1712511400,"Improving@Speech@Separation@with@Knowledge@Distilled@from@Self-supervised@Pre-trained@Models":1712511400,"Large-Scale@Self-Supervised@Speech@Representation@Learning@for@Automatic@Speaker@Verification":1712511399,"Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition":1712511399,"Local@Information@Modeling@with@Self-Attention@for@Speaker@Verification":1712511399,"Medical@Difficult@Airway@Detection@using@Speech@Technology":1712511399,"MLP-SVNET@@A@Multi-Layer@Perceptrons@Based@Network@for@Speaker@Verification":0,"Optimizing@Alignment@of@Speech@and@Language@Latent@Spaces@for@End-To-End@Speech@Recognition@and@Understanding":1712511399,"Optimizing@Data@Usage@for@Low-Resource@Speech@Recognition":1712511400,"Punctuation@Prediction@for@Streaming@On-Device@Speech@Recognition":1712511399,"Self-Knowledge@Distillation@via@Feature@Enhancement@for@Speaker@Verification":1712511399,"Skim@@Skipping@Memory@Lstm@for@Low-Latency@Real-Time@Continuous@Speech@Separation":0,"Summary@on@the@ICASSP@2022@Multi-Channel@Multi-Party@Meeting@Transcription@Grand@Challenge":1712511400,"Text-Informed@Knowledge@Distillation@for@Robust@Speech@Enhancement@and@Recognition":1712511399,"The@Conversational@Short-phrase@Speaker@Diarization@@CSSD@@Task@@Dataset@@Evaluation@Metric@and@Baselines":0,"The@Sjtu@System@For@Multimodal@Information@Based@Speech@Processing@Challenge@2021":1712511399,"The@X-Lance@Speaker@Diarization@System@for@the@Conversational@Short-phrase@Speaker@Diarization@Challenge@2022":1712511399,"Time-Domain@Audio-Visual@Speech@Separation@on@Low@Quality@Videos":1712511399,"WavLM@@Large-Scale@Self-Supervised@Pre-Training@for@Full@Stack@Speech@Processing":0},"checksum":"4463553866f6b3765a9891233173493f"},"03.research\/02.publications\/2022\/@Attentive@Feature@Fusion@for@Robust@Speaker@Verification@":{"key":"research\/publications\/2022\/@attentive@feature@fusion@for@robust@speaker@verification@","storage_key":"03.research\/02.publications\/2022\/@Attentive@Feature@Fusion@for@Robust@Speaker@Verification@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"91ad7cd5cf83b30a5e55d4331b30ae34"},"03.research\/02.publications\/2022\/@DF-ResNet@@Boosting@Speaker@Verification@Performance@with@Depth-First@Design@":{"key":"research\/publications\/2022\/@df-resnet@@boosting@speaker@verification@performance@with@depth-first@design@","storage_key":"03.research\/02.publications\/2022\/@DF-ResNet@@Boosting@Speaker@Verification@Performance@with@Depth-First@Design@","template":null,"storage_timestamp":0,"checksum":"01f4dffcb733848e65f6ec5c41925ce5"},"03.research\/02.publications\/2022\/@Dual@Path@Embedding@Learning@for@Speaker@Verification@with@Triplet@Attention@":{"key":"research\/publications\/2022\/@dual@path@embedding@learning@for@speaker@verification@with@triplet@attention@","storage_key":"03.research\/02.publications\/2022\/@Dual@Path@Embedding@Learning@for@Speaker@Verification@with@Triplet@Attention@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511399}},"checksum":"1459ab576acf30cdca4b085d6d97bc6e"},"03.research\/02.publications\/2022\/@Enroll-Aware@Attentive@Statistics@Pooling@for@Target@Speaker@Verification@":{"key":"research\/publications\/2022\/@enroll-aware@attentive@statistics@pooling@for@target@speaker@verification@","storage_key":"03.research\/02.publications\/2022\/@Enroll-Aware@Attentive@Statistics@Pooling@for@Target@Speaker@Verification@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"63c5b9301b49cf62cf9f9a0d3f767b71"},"03.research\/02.publications\/2022\/@ESPnet-SE++@@Speech@Enhancement@for@Robust@Speech@Recognition@@Translation@@and@Understanding@":{"key":"research\/publications\/2022\/@espnet-se++@@speech@enhancement@for@robust@speech@recognition@@translation@@and@understanding@","storage_key":"03.research\/02.publications\/2022\/@ESPnet-SE++@@Speech@Enhancement@for@Robust@Speech@Recognition@@Translation@@and@Understanding@","template":null,"storage_timestamp":0,"checksum":"c4e2e4787a8f43ee114417b2a55f909c"},"03.research\/02.publications\/2022\/@Knowledge@Transfer@and@Distillation@from@Autoregressive@to@Non-Autoregessive@Speech@Recognition@":{"key":"research\/publications\/2022\/@knowledge@transfer@and@distillation@from@autoregressive@to@non-autoregessive@speech@recognition@","storage_key":"03.research\/02.publications\/2022\/@Knowledge@Transfer@and@Distillation@from@Autoregressive@to@Non-Autoregessive@Speech@Recognition@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"ad8d1b5cbd9caf30f8a88e2cfaf58653"},"03.research\/02.publications\/2022\/@MSDWild@@Multi-modal@Speaker@Diarization@Dataset@in@the@Wild@":{"key":"research\/publications\/2022\/@msdwild@@multi-modal@speaker@diarization@dataset@in@the@wild@","storage_key":"03.research\/02.publications\/2022\/@MSDWild@@Multi-modal@Speaker@Diarization@Dataset@in@the@Wild@","template":null,"storage_timestamp":0,"checksum":"ab5d4202c97a04556b156ffe43636913"},"03.research\/02.publications\/2022\/@Self-Supervised@Speaker@Verification@Using@Dynamic@Loss-Gate@and@Label@Correction@":{"key":"research\/publications\/2022\/@self-supervised@speaker@verification@using@dynamic@loss-gate@and@label@correction@","storage_key":"03.research\/02.publications\/2022\/@Self-Supervised@Speaker@Verification@Using@Dynamic@Loss-Gate@and@Label@Correction@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"dcc5f3386bbedefedb736f6233d65c70"},"03.research\/02.publications\/2022\/@Separating@Long-Form@Speech@with@Group-wise@Permutation@Invariant@Training@":{"key":"research\/publications\/2022\/@separating@long-form@speech@with@group-wise@permutation@invariant@training@","storage_key":"03.research\/02.publications\/2022\/@Separating@Long-Form@Speech@with@Group-wise@Permutation@Invariant@Training@","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"0981a98f4e8f5d19c0210706f31029ba"},"03.research\/02.publications\/2022\/Dual-Path@Modeling@With@Memory@Embedding@Model@for@Continuous@Speech@Separation":{"key":"research\/publications\/2022\/dual-path@modeling@with@memory@embedding@model@for@continuous@speech@separation","storage_key":"03.research\/02.publications\/2022\/Dual-Path@Modeling@With@Memory@Embedding@Model@for@Continuous@Speech@Separation","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"f2cd2adb47449924359dbc725e202be9"},"03.research\/02.publications\/2022\/End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@in@a@Cocktail@Party":{"key":"research\/publications\/2022\/end-to-end@dereverberation@@beamforming@@and@speech@recognition@in@a@cocktail@party","storage_key":"03.research\/02.publications\/2022\/End-to-End@Dereverberation@@Beamforming@@and@Speech@Recognition@in@a@Cocktail@Party","template":null,"storage_timestamp":0,"checksum":"33a5890b58d1ad8aa8bf60ef30450712"},"03.research\/02.publications\/2022\/Exploring@Effective@Data@Utilization@for@Low-Resource@Speech@Recognition":{"key":"research\/publications\/2022\/exploring@effective@data@utilization@for@low-resource@speech@recognition","storage_key":"03.research\/02.publications\/2022\/Exploring@Effective@Data@Utilization@for@Low-Resource@Speech@Recognition","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"b9b3295483cfdb4a4eed6eceffa20a23"},"03.research\/02.publications\/2022\/Improving@Speech@Separation@with@Knowledge@Distilled@from@Self-supervised@Pre-trained@Models":{"key":"research\/publications\/2022\/improving@speech@separation@with@knowledge@distilled@from@self-supervised@pre-trained@models","storage_key":"03.research\/02.publications\/2022\/Improving@Speech@Separation@with@Knowledge@Distilled@from@Self-supervised@Pre-trained@Models","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"813abe8c01440f2c087d6874e5077084"},"03.research\/02.publications\/2022\/Large-Scale@Self-Supervised@Speech@Representation@Learning@for@Automatic@Speaker@Verification":{"key":"research\/publications\/2022\/large-scale@self-supervised@speech@representation@learning@for@automatic@speaker@verification","storage_key":"03.research\/02.publications\/2022\/Large-Scale@Self-Supervised@Speech@Representation@Learning@for@Automatic@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"795555b386260365e05ff1a3ff6fb96b"},"03.research\/02.publications\/2022\/Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition":{"key":"research\/publications\/2022\/layer-wise@fast@adaptation@for@end-to-end@multi-accent@speech@recognition","storage_key":"03.research\/02.publications\/2022\/Layer-Wise@Fast@Adaptation@for@End-to-End@Multi-Accent@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"e69bfb8391fa232b795729ae70d81b33"},"03.research\/02.publications\/2022\/Local@Information@Modeling@with@Self-Attention@for@Speaker@Verification":{"key":"research\/publications\/2022\/local@information@modeling@with@self-attention@for@speaker@verification","storage_key":"03.research\/02.publications\/2022\/Local@Information@Modeling@with@Self-Attention@for@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"ebbcc607593d85a5181cade82b92c512"},"03.research\/02.publications\/2022\/Medical@Difficult@Airway@Detection@using@Speech@Technology":{"key":"research\/publications\/2022\/medical@difficult@airway@detection@using@speech@technology","storage_key":"03.research\/02.publications\/2022\/Medical@Difficult@Airway@Detection@using@Speech@Technology","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"84535d71fb094ed7ebb3bc29ad222263"},"03.research\/02.publications\/2022\/MLP-SVNET@@A@Multi-Layer@Perceptrons@Based@Network@for@Speaker@Verification":{"key":"research\/publications\/2022\/mlp-svnet@@a@multi-layer@perceptrons@based@network@for@speaker@verification","storage_key":"03.research\/02.publications\/2022\/MLP-SVNET@@A@Multi-Layer@Perceptrons@Based@Network@for@Speaker@Verification","template":null,"storage_timestamp":0,"checksum":"0dc8a721591eb6c42972d5f2a541b71b"},"03.research\/02.publications\/2022\/Optimizing@Alignment@of@Speech@and@Language@Latent@Spaces@for@End-To-End@Speech@Recognition@and@Understanding":{"key":"research\/publications\/2022\/optimizing@alignment@of@speech@and@language@latent@spaces@for@end-to-end@speech@recognition@and@understanding","storage_key":"03.research\/02.publications\/2022\/Optimizing@Alignment@of@Speech@and@Language@Latent@Spaces@for@End-To-End@Speech@Recognition@and@Understanding","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"7da1ff0686b961364550f0f82186eec5"},"03.research\/02.publications\/2022\/Optimizing@Data@Usage@for@Low-Resource@Speech@Recognition":{"key":"research\/publications\/2022\/optimizing@data@usage@for@low-resource@speech@recognition","storage_key":"03.research\/02.publications\/2022\/Optimizing@Data@Usage@for@Low-Resource@Speech@Recognition","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"23c16678471222d5e5f715fc370ec89d"},"03.research\/02.publications\/2022\/Punctuation@Prediction@for@Streaming@On-Device@Speech@Recognition":{"key":"research\/publications\/2022\/punctuation@prediction@for@streaming@on-device@speech@recognition","storage_key":"03.research\/02.publications\/2022\/Punctuation@Prediction@for@Streaming@On-Device@Speech@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"349ea40ccf99886ce295901866341435"},"03.research\/02.publications\/2022\/Self-Knowledge@Distillation@via@Feature@Enhancement@for@Speaker@Verification":{"key":"research\/publications\/2022\/self-knowledge@distillation@via@feature@enhancement@for@speaker@verification","storage_key":"03.research\/02.publications\/2022\/Self-Knowledge@Distillation@via@Feature@Enhancement@for@Speaker@Verification","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"96d9f04b79e080084ba8a642feeac44b"},"03.research\/02.publications\/2022\/Skim@@Skipping@Memory@Lstm@for@Low-Latency@Real-Time@Continuous@Speech@Separation":{"key":"research\/publications\/2022\/skim@@skipping@memory@lstm@for@low-latency@real-time@continuous@speech@separation","storage_key":"03.research\/02.publications\/2022\/Skim@@Skipping@Memory@Lstm@for@Low-Latency@Real-Time@Continuous@Speech@Separation","template":null,"storage_timestamp":0,"checksum":"e2362f647b8a1465c6d90d2828b7af5e"},"03.research\/02.publications\/2022\/Summary@on@the@ICASSP@2022@Multi-Channel@Multi-Party@Meeting@Transcription@Grand@Challenge":{"key":"research\/publications\/2022\/summary@on@the@icassp@2022@multi-channel@multi-party@meeting@transcription@grand@challenge","storage_key":"03.research\/02.publications\/2022\/Summary@on@the@ICASSP@2022@Multi-Channel@Multi-Party@Meeting@Transcription@Grand@Challenge","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"0b9991ef2754754bacdd61421c5c0997"},"03.research\/02.publications\/2022\/Text-Informed@Knowledge@Distillation@for@Robust@Speech@Enhancement@and@Recognition":{"key":"research\/publications\/2022\/text-informed@knowledge@distillation@for@robust@speech@enhancement@and@recognition","storage_key":"03.research\/02.publications\/2022\/Text-Informed@Knowledge@Distillation@for@Robust@Speech@Enhancement@and@Recognition","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"ff33684339704ecdb5a4a003e6dcb57e"},"03.research\/02.publications\/2022\/The@Conversational@Short-phrase@Speaker@Diarization@@CSSD@@Task@@Dataset@@Evaluation@Metric@and@Baselines":{"key":"research\/publications\/2022\/the@conversational@short-phrase@speaker@diarization@@cssd@@task@@dataset@@evaluation@metric@and@baselines","storage_key":"03.research\/02.publications\/2022\/The@Conversational@Short-phrase@Speaker@Diarization@@CSSD@@Task@@Dataset@@Evaluation@Metric@and@Baselines","template":null,"storage_timestamp":0,"checksum":"3fe7abcb980b87be43e3b101d3501800"},"03.research\/02.publications\/2022\/The@Sjtu@System@For@Multimodal@Information@Based@Speech@Processing@Challenge@2021":{"key":"research\/publications\/2022\/the@sjtu@system@for@multimodal@information@based@speech@processing@challenge@2021","storage_key":"03.research\/02.publications\/2022\/The@Sjtu@System@For@Multimodal@Information@Based@Speech@Processing@Challenge@2021","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"55642f7289cff29c3e11cebb027fb6d4"},"03.research\/02.publications\/2022\/The@X-Lance@Speaker@Diarization@System@for@the@Conversational@Short-phrase@Speaker@Diarization@Challenge@2022":{"key":"research\/publications\/2022\/the@x-lance@speaker@diarization@system@for@the@conversational@short-phrase@speaker@diarization@challenge@2022","storage_key":"03.research\/02.publications\/2022\/The@X-Lance@Speaker@Diarization@System@for@the@Conversational@Short-phrase@Speaker@Diarization@Challenge@2022","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"614a7266638c93925c75a862752a61bb"},"03.research\/02.publications\/2022\/Time-Domain@Audio-Visual@Speech@Separation@on@Low@Quality@Videos":{"key":"research\/publications\/2022\/time-domain@audio-visual@speech@separation@on@low@quality@videos","storage_key":"03.research\/02.publications\/2022\/Time-Domain@Audio-Visual@Speech@Separation@on@Low@Quality@Videos","template":"default","storage_timestamp":1712511399,"markdown":{"":{"default":1712511399}},"checksum":"ced66a6e59f680618dc2855f607bad66"},"03.research\/02.publications\/2022\/WavLM@@Large-Scale@Self-Supervised@Pre-Training@for@Full@Stack@Speech@Processing":{"key":"research\/publications\/2022\/wavlm@@large-scale@self-supervised@pre-training@for@full@stack@speech@processing","storage_key":"03.research\/02.publications\/2022\/WavLM@@Large-Scale@Self-Supervised@Pre-Training@for@Full@Stack@Speech@Processing","template":null,"storage_timestamp":0,"checksum":"a99eab0e56536504c23a8d391df58411"},"03.research\/02.publications\/2023":{"key":"research\/publications\/2023","storage_key":"03.research\/02.publications\/2023","template":null,"storage_timestamp":1712511400,"children":{"@Adapting@Multi-Lingual@ASR@Models@for@Handling@Multiple@Talkers@":1712511400,"@Adaptive@Neural@Network@Quantization@For@Lightweight@Speaker@Verification@":1712511400,"@Attention-based@Encoder-Decoder@Network@for@End-to-End@Neural@Speaker@Diarization@with@Target@Speaker@Attractor@":1712511400,"@Build@a@SRE@Challenge@System@@Lessons@from@VoxSRC@2022@and@CNSRC@2022@":0,"@ECAPA++@@Fine-grained@Deep@Embedding@Learning@for@TDNN@Based@Speaker@Verification@":0,"@Extremely@Low@Bit@Quantization@for@Mobile@Speaker@Verification@Systems@Under@1MB@Memory@":1712511400,"@Fast@and@Efficient@Multilingual@Self-Supervised@Pre-training@for@Low-Resource@Speech@Recognition@":1712511400,"@Overlap@Aware@Continuous@Speech@Separation@without@Permutation@Invariant@Training@":1712511400,"@Reversible@Neural@Networks@for@Memory-Efficient@Speaker@Verification@":1712511400,"@Text@Only@Domain@Adaptation@with@Phoneme@Guided@Data@Splicing@for@End-to-End@Speech@Recognition@":1712511400,"@UniSplice@@Universal@Cross-Lingual@Data@Splicing@for@Low-Resource@ASR@":0,"@Weakly-Supervised@Speech@Pre-training@@A@Case@Study@on@Target@Speech@Recognition@":0,"A@Comprehensive@Study@on@Self-Supervised@Distillation@for@Speaker@Representation@Learning":1712511400,"Adaptive@Large@Margin@Fine-Tuning@For@Robust@Speaker@Verification":1712511400,"Code-Switching@Text@Generation@and@Injection@in@Mandarin-English@ASR":1712511400,"Depth-First@Neural@Architecture@With@Attentive@Feature@Fusion@for@Efficient@Speaker@Verification":1712511400,"Efficient@Text-Only@Domain@Adaptation@For@CTC-Based@ASR":1712511400,"End-to-End@Multi-Speaker@ASR@with@Independent@Vector@Analysis":1712511400,"Exploring@Binary@Classification@Loss@for@Speaker@Verification":1712511400,"Exploring@the@Integration@of@Speech@Separation@and@Recognition@with@Self-Supervised@Learning@Representation":1712511400,"Exploring@Time-Frequency@Domain@Target@Speaker@Extraction@For@Causal@and@Non-Causal@Processing":1712511400,"Factorized@AED@@Factorized@Attention-Based@Encoder-Decoder@for@Text-Only@Domain@Adaptive@ASR":0,"FAT-HuBERT@@Front-End@Adaptive@Training@of@Hidden-Unit@BERT@For@Distortion-Invariant@Robust@Speech@Recognition":0,"HuBERT-AGG@@Aggregated@Representation@Distillation@of@Hidden-Unit@Bert@for@Robust@Speech@Recognition":0,"Improving@Dino-Based@Self-Supervised@Speaker@Verification@with@Progressive@Cluster-Aware@Training":1712511400,"Improving@Speech@Enhancement@Using@Audio@Tagging@Knowledge@From@Pre-Trained@Representations@and@Multi-Task@Learning":1712511400,"Joint@Discriminator@and@Transfer@Based@Fast@Domain@Adaptation@For@End-To-End@Speech@Recognition":1712511400,"Light-Weight@Visualvoice@@Neural@Network@Quantization@On@Audio@Visual@Speech@Separation":0,"LongFNT@@Long-Form@Speech@Recognition@with@Factorized@Neural@Transducer":0,"Lowbit@Neural@Network@Quantization@for@Speaker@Verification":1712511400,"Multi-Speaker@End-to-End@Multi-Modal@Speaker@Diarization@System@for@the@MISP@2022@Challenge":1712511400,"Predictive@Skim@@Contrastive@Predictive@Coding@for@Low-Latency@Online@Speech@Separation":0,"Robust@Audio-Visual@ASR@with@Unified@Cross-Modal@Attention":1712511400,"Target@Sound@Extraction@with@Variable@Cross-Modality@Clues":1712511400,"The@Second@Multi-Channel@Multi-Party@Meeting@Transcription@Challenge@@M2MeT@2@0@@@A@Benchmark@for@Speaker-Attributed@ASR":0,"Toward@Universal@Speech@Enhancement@For@Diverse@Input@Conditions":1712511400,"Wespeaker@@A@Research@and@Production@Oriented@Speaker@Embedding@Learning@Toolkit":0},"checksum":"63e8a1a041d2f8ce971e456ee7a98b8d"},"03.research\/02.publications\/2023\/@Adapting@Multi-Lingual@ASR@Models@for@Handling@Multiple@Talkers@":{"key":"research\/publications\/2023\/@adapting@multi-lingual@asr@models@for@handling@multiple@talkers@","storage_key":"03.research\/02.publications\/2023\/@Adapting@Multi-Lingual@ASR@Models@for@Handling@Multiple@Talkers@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"1db28a3dc343e132192fb44ea8613de5"},"03.research\/02.publications\/2023\/@Adaptive@Neural@Network@Quantization@For@Lightweight@Speaker@Verification@":{"key":"research\/publications\/2023\/@adaptive@neural@network@quantization@for@lightweight@speaker@verification@","storage_key":"03.research\/02.publications\/2023\/@Adaptive@Neural@Network@Quantization@For@Lightweight@Speaker@Verification@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"d50252deae1ffe8e5cd3cc005a62fbd0"},"03.research\/02.publications\/2023\/@Attention-based@Encoder-Decoder@Network@for@End-to-End@Neural@Speaker@Diarization@with@Target@Speaker@Attractor@":{"key":"research\/publications\/2023\/@attention-based@encoder-decoder@network@for@end-to-end@neural@speaker@diarization@with@target@speaker@attractor@","storage_key":"03.research\/02.publications\/2023\/@Attention-based@Encoder-Decoder@Network@for@End-to-End@Neural@Speaker@Diarization@with@Target@Speaker@Attractor@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"1e58b5d188b2475a1b5d0655bdde4079"},"03.research\/02.publications\/2023\/@Build@a@SRE@Challenge@System@@Lessons@from@VoxSRC@2022@and@CNSRC@2022@":{"key":"research\/publications\/2023\/@build@a@sre@challenge@system@@lessons@from@voxsrc@2022@and@cnsrc@2022@","storage_key":"03.research\/02.publications\/2023\/@Build@a@SRE@Challenge@System@@Lessons@from@VoxSRC@2022@and@CNSRC@2022@","template":null,"storage_timestamp":0,"checksum":"e38a8c6ff5e6f97c2612a07466076a59"},"03.research\/02.publications\/2023\/@ECAPA++@@Fine-grained@Deep@Embedding@Learning@for@TDNN@Based@Speaker@Verification@":{"key":"research\/publications\/2023\/@ecapa++@@fine-grained@deep@embedding@learning@for@tdnn@based@speaker@verification@","storage_key":"03.research\/02.publications\/2023\/@ECAPA++@@Fine-grained@Deep@Embedding@Learning@for@TDNN@Based@Speaker@Verification@","template":null,"storage_timestamp":0,"checksum":"bcc8b07f197e34cab3688a2f36fcae7a"},"03.research\/02.publications\/2023\/@Extremely@Low@Bit@Quantization@for@Mobile@Speaker@Verification@Systems@Under@1MB@Memory@":{"key":"research\/publications\/2023\/@extremely@low@bit@quantization@for@mobile@speaker@verification@systems@under@1mb@memory@","storage_key":"03.research\/02.publications\/2023\/@Extremely@Low@Bit@Quantization@for@Mobile@Speaker@Verification@Systems@Under@1MB@Memory@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"08979ed265bb9bb017155df498012c97"},"03.research\/02.publications\/2023\/@Fast@and@Efficient@Multilingual@Self-Supervised@Pre-training@for@Low-Resource@Speech@Recognition@":{"key":"research\/publications\/2023\/@fast@and@efficient@multilingual@self-supervised@pre-training@for@low-resource@speech@recognition@","storage_key":"03.research\/02.publications\/2023\/@Fast@and@Efficient@Multilingual@Self-Supervised@Pre-training@for@Low-Resource@Speech@Recognition@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"d2dd50ccec39055333ee2c8cfa0cf33f"},"03.research\/02.publications\/2023\/@Overlap@Aware@Continuous@Speech@Separation@without@Permutation@Invariant@Training@":{"key":"research\/publications\/2023\/@overlap@aware@continuous@speech@separation@without@permutation@invariant@training@","storage_key":"03.research\/02.publications\/2023\/@Overlap@Aware@Continuous@Speech@Separation@without@Permutation@Invariant@Training@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"aeddafce2f6dc92bd984a8d95621c36b"},"03.research\/02.publications\/2023\/@Reversible@Neural@Networks@for@Memory-Efficient@Speaker@Verification@":{"key":"research\/publications\/2023\/@reversible@neural@networks@for@memory-efficient@speaker@verification@","storage_key":"03.research\/02.publications\/2023\/@Reversible@Neural@Networks@for@Memory-Efficient@Speaker@Verification@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"a1f91bad0e07aaf0d7c1552f107e9d14"},"03.research\/02.publications\/2023\/@Text@Only@Domain@Adaptation@with@Phoneme@Guided@Data@Splicing@for@End-to-End@Speech@Recognition@":{"key":"research\/publications\/2023\/@text@only@domain@adaptation@with@phoneme@guided@data@splicing@for@end-to-end@speech@recognition@","storage_key":"03.research\/02.publications\/2023\/@Text@Only@Domain@Adaptation@with@Phoneme@Guided@Data@Splicing@for@End-to-End@Speech@Recognition@","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"efd955e8ea4493d997e684c1ed91d107"},"03.research\/02.publications\/2023\/@UniSplice@@Universal@Cross-Lingual@Data@Splicing@for@Low-Resource@ASR@":{"key":"research\/publications\/2023\/@unisplice@@universal@cross-lingual@data@splicing@for@low-resource@asr@","storage_key":"03.research\/02.publications\/2023\/@UniSplice@@Universal@Cross-Lingual@Data@Splicing@for@Low-Resource@ASR@","template":null,"storage_timestamp":0,"checksum":"81c2bb718f560620b5c2ae2e4b18c3f8"},"03.research\/02.publications\/2023\/@Weakly-Supervised@Speech@Pre-training@@A@Case@Study@on@Target@Speech@Recognition@":{"key":"research\/publications\/2023\/@weakly-supervised@speech@pre-training@@a@case@study@on@target@speech@recognition@","storage_key":"03.research\/02.publications\/2023\/@Weakly-Supervised@Speech@Pre-training@@A@Case@Study@on@Target@Speech@Recognition@","template":null,"storage_timestamp":0,"checksum":"0f8ed80e29675a387ccbba223ffeebb3"},"03.research\/02.publications\/2023\/A@Comprehensive@Study@on@Self-Supervised@Distillation@for@Speaker@Representation@Learning":{"key":"research\/publications\/2023\/a@comprehensive@study@on@self-supervised@distillation@for@speaker@representation@learning","storage_key":"03.research\/02.publications\/2023\/A@Comprehensive@Study@on@Self-Supervised@Distillation@for@Speaker@Representation@Learning","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"d860d3b74703cd9610a944a70fb65eea"},"03.research\/02.publications\/2023\/Adaptive@Large@Margin@Fine-Tuning@For@Robust@Speaker@Verification":{"key":"research\/publications\/2023\/adaptive@large@margin@fine-tuning@for@robust@speaker@verification","storage_key":"03.research\/02.publications\/2023\/Adaptive@Large@Margin@Fine-Tuning@For@Robust@Speaker@Verification","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"30cfc5d301d51cd301dc48442976b713"},"03.research\/02.publications\/2023\/Code-Switching@Text@Generation@and@Injection@in@Mandarin-English@ASR":{"key":"research\/publications\/2023\/code-switching@text@generation@and@injection@in@mandarin-english@asr","storage_key":"03.research\/02.publications\/2023\/Code-Switching@Text@Generation@and@Injection@in@Mandarin-English@ASR","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"19fc3623dceead1d5a43234d32297c7c"},"03.research\/02.publications\/2023\/Depth-First@Neural@Architecture@With@Attentive@Feature@Fusion@for@Efficient@Speaker@Verification":{"key":"research\/publications\/2023\/depth-first@neural@architecture@with@attentive@feature@fusion@for@efficient@speaker@verification","storage_key":"03.research\/02.publications\/2023\/Depth-First@Neural@Architecture@With@Attentive@Feature@Fusion@for@Efficient@Speaker@Verification","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"866397f2926e1a1a56d4e13a9021ba57"},"03.research\/02.publications\/2023\/Efficient@Text-Only@Domain@Adaptation@For@CTC-Based@ASR":{"key":"research\/publications\/2023\/efficient@text-only@domain@adaptation@for@ctc-based@asr","storage_key":"03.research\/02.publications\/2023\/Efficient@Text-Only@Domain@Adaptation@For@CTC-Based@ASR","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"e85d141275452ab3fe71473a1cc8e554"},"03.research\/02.publications\/2023\/End-to-End@Multi-Speaker@ASR@with@Independent@Vector@Analysis":{"key":"research\/publications\/2023\/end-to-end@multi-speaker@asr@with@independent@vector@analysis","storage_key":"03.research\/02.publications\/2023\/End-to-End@Multi-Speaker@ASR@with@Independent@Vector@Analysis","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"86faf723155a0cb8c1c734bf557f8e70"},"03.research\/02.publications\/2023\/Exploring@Binary@Classification@Loss@for@Speaker@Verification":{"key":"research\/publications\/2023\/exploring@binary@classification@loss@for@speaker@verification","storage_key":"03.research\/02.publications\/2023\/Exploring@Binary@Classification@Loss@for@Speaker@Verification","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"826f80b3775cf31ccb751062df266bf5"},"03.research\/02.publications\/2023\/Exploring@the@Integration@of@Speech@Separation@and@Recognition@with@Self-Supervised@Learning@Representation":{"key":"research\/publications\/2023\/exploring@the@integration@of@speech@separation@and@recognition@with@self-supervised@learning@representation","storage_key":"03.research\/02.publications\/2023\/Exploring@the@Integration@of@Speech@Separation@and@Recognition@with@Self-Supervised@Learning@Representation","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"1b472bccf535cf673ec6ce3f4c7943f6"},"03.research\/02.publications\/2023\/Exploring@Time-Frequency@Domain@Target@Speaker@Extraction@For@Causal@and@Non-Causal@Processing":{"key":"research\/publications\/2023\/exploring@time-frequency@domain@target@speaker@extraction@for@causal@and@non-causal@processing","storage_key":"03.research\/02.publications\/2023\/Exploring@Time-Frequency@Domain@Target@Speaker@Extraction@For@Causal@and@Non-Causal@Processing","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"75cb13664acce4ae1f152feb5a0502a3"},"03.research\/02.publications\/2023\/Factorized@AED@@Factorized@Attention-Based@Encoder-Decoder@for@Text-Only@Domain@Adaptive@ASR":{"key":"research\/publications\/2023\/factorized@aed@@factorized@attention-based@encoder-decoder@for@text-only@domain@adaptive@asr","storage_key":"03.research\/02.publications\/2023\/Factorized@AED@@Factorized@Attention-Based@Encoder-Decoder@for@Text-Only@Domain@Adaptive@ASR","template":null,"storage_timestamp":0,"checksum":"5af5c2b0bb884aec06471e3f5ed71c58"},"03.research\/02.publications\/2023\/FAT-HuBERT@@Front-End@Adaptive@Training@of@Hidden-Unit@BERT@For@Distortion-Invariant@Robust@Speech@Recognition":{"key":"research\/publications\/2023\/fat-hubert@@front-end@adaptive@training@of@hidden-unit@bert@for@distortion-invariant@robust@speech@recognition","storage_key":"03.research\/02.publications\/2023\/FAT-HuBERT@@Front-End@Adaptive@Training@of@Hidden-Unit@BERT@For@Distortion-Invariant@Robust@Speech@Recognition","template":null,"storage_timestamp":0,"checksum":"55799c65863474a6d036860bf8e951d9"},"03.research\/02.publications\/2023\/HuBERT-AGG@@Aggregated@Representation@Distillation@of@Hidden-Unit@Bert@for@Robust@Speech@Recognition":{"key":"research\/publications\/2023\/hubert-agg@@aggregated@representation@distillation@of@hidden-unit@bert@for@robust@speech@recognition","storage_key":"03.research\/02.publications\/2023\/HuBERT-AGG@@Aggregated@Representation@Distillation@of@Hidden-Unit@Bert@for@Robust@Speech@Recognition","template":null,"storage_timestamp":0,"checksum":"4b307586c954ac5d56d7df89baa1c7c6"},"03.research\/02.publications\/2023\/Improving@Dino-Based@Self-Supervised@Speaker@Verification@with@Progressive@Cluster-Aware@Training":{"key":"research\/publications\/2023\/improving@dino-based@self-supervised@speaker@verification@with@progressive@cluster-aware@training","storage_key":"03.research\/02.publications\/2023\/Improving@Dino-Based@Self-Supervised@Speaker@Verification@with@Progressive@Cluster-Aware@Training","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"e4aaff54be5510718384a31029b4e689"},"03.research\/02.publications\/2023\/Improving@Speech@Enhancement@Using@Audio@Tagging@Knowledge@From@Pre-Trained@Representations@and@Multi-Task@Learning":{"key":"research\/publications\/2023\/improving@speech@enhancement@using@audio@tagging@knowledge@from@pre-trained@representations@and@multi-task@learning","storage_key":"03.research\/02.publications\/2023\/Improving@Speech@Enhancement@Using@Audio@Tagging@Knowledge@From@Pre-Trained@Representations@and@Multi-Task@Learning","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"d043094f7ccd9cb3f4764e3918586f1b"},"03.research\/02.publications\/2023\/Joint@Discriminator@and@Transfer@Based@Fast@Domain@Adaptation@For@End-To-End@Speech@Recognition":{"key":"research\/publications\/2023\/joint@discriminator@and@transfer@based@fast@domain@adaptation@for@end-to-end@speech@recognition","storage_key":"03.research\/02.publications\/2023\/Joint@Discriminator@and@Transfer@Based@Fast@Domain@Adaptation@For@End-To-End@Speech@Recognition","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"0ffb3d414bdd4aa0b677e267eea9b816"},"03.research\/02.publications\/2023\/Light-Weight@Visualvoice@@Neural@Network@Quantization@On@Audio@Visual@Speech@Separation":{"key":"research\/publications\/2023\/light-weight@visualvoice@@neural@network@quantization@on@audio@visual@speech@separation","storage_key":"03.research\/02.publications\/2023\/Light-Weight@Visualvoice@@Neural@Network@Quantization@On@Audio@Visual@Speech@Separation","template":null,"storage_timestamp":0,"checksum":"d09a17fb01386d3869eabca0bda1b218"},"03.research\/02.publications\/2023\/LongFNT@@Long-Form@Speech@Recognition@with@Factorized@Neural@Transducer":{"key":"research\/publications\/2023\/longfnt@@long-form@speech@recognition@with@factorized@neural@transducer","storage_key":"03.research\/02.publications\/2023\/LongFNT@@Long-Form@Speech@Recognition@with@Factorized@Neural@Transducer","template":null,"storage_timestamp":0,"checksum":"1675d97473bfb2676d517b4ab38890d0"},"03.research\/02.publications\/2023\/Lowbit@Neural@Network@Quantization@for@Speaker@Verification":{"key":"research\/publications\/2023\/lowbit@neural@network@quantization@for@speaker@verification","storage_key":"03.research\/02.publications\/2023\/Lowbit@Neural@Network@Quantization@for@Speaker@Verification","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"c0356c9f0e9561112d5793a83bc80bff"},"03.research\/02.publications\/2023\/Multi-Speaker@End-to-End@Multi-Modal@Speaker@Diarization@System@for@the@MISP@2022@Challenge":{"key":"research\/publications\/2023\/multi-speaker@end-to-end@multi-modal@speaker@diarization@system@for@the@misp@2022@challenge","storage_key":"03.research\/02.publications\/2023\/Multi-Speaker@End-to-End@Multi-Modal@Speaker@Diarization@System@for@the@MISP@2022@Challenge","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"91b9d847a9f53e1d797e0c83b52a7ed7"},"03.research\/02.publications\/2023\/Predictive@Skim@@Contrastive@Predictive@Coding@for@Low-Latency@Online@Speech@Separation":{"key":"research\/publications\/2023\/predictive@skim@@contrastive@predictive@coding@for@low-latency@online@speech@separation","storage_key":"03.research\/02.publications\/2023\/Predictive@Skim@@Contrastive@Predictive@Coding@for@Low-Latency@Online@Speech@Separation","template":null,"storage_timestamp":0,"checksum":"8f448fb8a3b8c5c235c58131f93bfb3c"},"03.research\/02.publications\/2023\/Robust@Audio-Visual@ASR@with@Unified@Cross-Modal@Attention":{"key":"research\/publications\/2023\/robust@audio-visual@asr@with@unified@cross-modal@attention","storage_key":"03.research\/02.publications\/2023\/Robust@Audio-Visual@ASR@with@Unified@Cross-Modal@Attention","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"e2bc25aaa064e296337f08c23c179754"},"03.research\/02.publications\/2023\/Target@Sound@Extraction@with@Variable@Cross-Modality@Clues":{"key":"research\/publications\/2023\/target@sound@extraction@with@variable@cross-modality@clues","storage_key":"03.research\/02.publications\/2023\/Target@Sound@Extraction@with@Variable@Cross-Modality@Clues","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"62bcd86845ecbd6e2c741a597a78977f"},"03.research\/02.publications\/2023\/The@Second@Multi-Channel@Multi-Party@Meeting@Transcription@Challenge@@M2MeT@2@0@@@A@Benchmark@for@Speaker-Attributed@ASR":{"key":"research\/publications\/2023\/the@second@multi-channel@multi-party@meeting@transcription@challenge@@m2met@2@0@@@a@benchmark@for@speaker-attributed@asr","storage_key":"03.research\/02.publications\/2023\/The@Second@Multi-Channel@Multi-Party@Meeting@Transcription@Challenge@@M2MeT@2@0@@@A@Benchmark@for@Speaker-Attributed@ASR","template":null,"storage_timestamp":0,"checksum":"2c55a95a022f4ad7310040a60dda59be"},"03.research\/02.publications\/2023\/Toward@Universal@Speech@Enhancement@For@Diverse@Input@Conditions":{"key":"research\/publications\/2023\/toward@universal@speech@enhancement@for@diverse@input@conditions","storage_key":"03.research\/02.publications\/2023\/Toward@Universal@Speech@Enhancement@For@Diverse@Input@Conditions","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"8b79ae90bae620be0c9aab6bca53c72e"},"03.research\/02.publications\/2023\/Wespeaker@@A@Research@and@Production@Oriented@Speaker@Embedding@Learning@Toolkit":{"key":"research\/publications\/2023\/wespeaker@@a@research@and@production@oriented@speaker@embedding@learning@toolkit","storage_key":"03.research\/02.publications\/2023\/Wespeaker@@A@Research@and@Production@Oriented@Speaker@Embedding@Learning@Toolkit","template":null,"storage_timestamp":0,"checksum":"c460aba43973be2c7fe1a3c1584d6711"},"03.research\/02.publications\/2024":{"key":"research\/publications\/2024","storage_key":"03.research\/02.publications\/2024","template":null,"storage_timestamp":1712511400,"children":{"Advanced@Long-Content@Speech@Recognition@With@Factorized@Neural@Transducer":1712511400,"Attention-Based@Encoder-Decoder@End-to-End@Neural@Diarization@With@Embedding@Enhancer":1712511400,"Exploring@Large@Scale@Pre-Trained@Models@for@Robust@Machine@Anomalous@Sound@Detection":1712511400,"Generation-Based@Target@Speech@Extraction@with@Speech@Discretization@and@Vocoder":1712511400,"Improving@Design@of@Input@Condition@Invariant@Speech@Enhancement":1712511400,"Leveraging@in-the-wild@Data@for@Effective@Self-supervised@Pretraining@in@Speaker@Recognition":1712511400,"One-Shot@Sensitivity-Aware@Mixed@Sparsity@Pruning@for@Large@Language@Models":1712511400,"Prompt-Driven@Target@Speech@Diarization":1712511400,"Robust@Cross-Domain@Speaker@Verification@with@Multi-Level@Domain@Adapters":1712511400,"Self-Supervised@Learning@With@Cluster-Aware-DINO@for@High-Performance@Robust@Speaker@Verification":1712511400,"Unified@Cross-Modal@Attention@@Robust@Audio-Visual@Speech@Recognition@and@Beyond":0,"Universal@Cross-Lingual@Data@Generation@for@Low@Resource@ASR":1712511400},"checksum":"ca0b1e7faf9b3289864932458be6da96"},"03.research\/02.publications\/2024\/Advanced@Long-Content@Speech@Recognition@With@Factorized@Neural@Transducer":{"key":"research\/publications\/2024\/advanced@long-content@speech@recognition@with@factorized@neural@transducer","storage_key":"03.research\/02.publications\/2024\/Advanced@Long-Content@Speech@Recognition@With@Factorized@Neural@Transducer","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"307b3b59bfad47c714885e71808107b6"},"03.research\/02.publications\/2024\/Attention-Based@Encoder-Decoder@End-to-End@Neural@Diarization@With@Embedding@Enhancer":{"key":"research\/publications\/2024\/attention-based@encoder-decoder@end-to-end@neural@diarization@with@embedding@enhancer","storage_key":"03.research\/02.publications\/2024\/Attention-Based@Encoder-Decoder@End-to-End@Neural@Diarization@With@Embedding@Enhancer","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"ddefc20dfc8f9817e315ab4a2b6f99fb"},"03.research\/02.publications\/2024\/Exploring@Large@Scale@Pre-Trained@Models@for@Robust@Machine@Anomalous@Sound@Detection":{"key":"research\/publications\/2024\/exploring@large@scale@pre-trained@models@for@robust@machine@anomalous@sound@detection","storage_key":"03.research\/02.publications\/2024\/Exploring@Large@Scale@Pre-Trained@Models@for@Robust@Machine@Anomalous@Sound@Detection","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"fb199c068ef57581600659965bfab263"},"03.research\/02.publications\/2024\/Generation-Based@Target@Speech@Extraction@with@Speech@Discretization@and@Vocoder":{"key":"research\/publications\/2024\/generation-based@target@speech@extraction@with@speech@discretization@and@vocoder","storage_key":"03.research\/02.publications\/2024\/Generation-Based@Target@Speech@Extraction@with@Speech@Discretization@and@Vocoder","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"7349f66587501cc358c2bfa2a40d931f"},"03.research\/02.publications\/2024\/Improving@Design@of@Input@Condition@Invariant@Speech@Enhancement":{"key":"research\/publications\/2024\/improving@design@of@input@condition@invariant@speech@enhancement","storage_key":"03.research\/02.publications\/2024\/Improving@Design@of@Input@Condition@Invariant@Speech@Enhancement","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"8f08af7eedb46c00ec01f37c989fb0ed"},"03.research\/02.publications\/2024\/Leveraging@in-the-wild@Data@for@Effective@Self-supervised@Pretraining@in@Speaker@Recognition":{"key":"research\/publications\/2024\/leveraging@in-the-wild@data@for@effective@self-supervised@pretraining@in@speaker@recognition","storage_key":"03.research\/02.publications\/2024\/Leveraging@in-the-wild@Data@for@Effective@Self-supervised@Pretraining@in@Speaker@Recognition","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"f25fa4c5399abe2ff17ae0299156b9c7"},"03.research\/02.publications\/2024\/One-Shot@Sensitivity-Aware@Mixed@Sparsity@Pruning@for@Large@Language@Models":{"key":"research\/publications\/2024\/one-shot@sensitivity-aware@mixed@sparsity@pruning@for@large@language@models","storage_key":"03.research\/02.publications\/2024\/One-Shot@Sensitivity-Aware@Mixed@Sparsity@Pruning@for@Large@Language@Models","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"ba533a71ef313bd42fd112159ddea2da"},"03.research\/02.publications\/2024\/Prompt-Driven@Target@Speech@Diarization":{"key":"research\/publications\/2024\/prompt-driven@target@speech@diarization","storage_key":"03.research\/02.publications\/2024\/Prompt-Driven@Target@Speech@Diarization","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"59de11929404a4d8d90de908548b3c07"},"03.research\/02.publications\/2024\/Robust@Cross-Domain@Speaker@Verification@with@Multi-Level@Domain@Adapters":{"key":"research\/publications\/2024\/robust@cross-domain@speaker@verification@with@multi-level@domain@adapters","storage_key":"03.research\/02.publications\/2024\/Robust@Cross-Domain@Speaker@Verification@with@Multi-Level@Domain@Adapters","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"d03898ac9df6c411d0e873cc352d8f4f"},"03.research\/02.publications\/2024\/Self-Supervised@Learning@With@Cluster-Aware-DINO@for@High-Performance@Robust@Speaker@Verification":{"key":"research\/publications\/2024\/self-supervised@learning@with@cluster-aware-dino@for@high-performance@robust@speaker@verification","storage_key":"03.research\/02.publications\/2024\/Self-Supervised@Learning@With@Cluster-Aware-DINO@for@High-Performance@Robust@Speaker@Verification","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"01f98ccb97a7a02fabf54ef2e9039185"},"03.research\/02.publications\/2024\/Unified@Cross-Modal@Attention@@Robust@Audio-Visual@Speech@Recognition@and@Beyond":{"key":"research\/publications\/2024\/unified@cross-modal@attention@@robust@audio-visual@speech@recognition@and@beyond","storage_key":"03.research\/02.publications\/2024\/Unified@Cross-Modal@Attention@@Robust@Audio-Visual@Speech@Recognition@and@Beyond","template":null,"storage_timestamp":0,"checksum":"58d013c6d4d7a64b67168baae5e23ca4"},"03.research\/02.publications\/2024\/Universal@Cross-Lingual@Data@Generation@for@Low@Resource@ASR":{"key":"research\/publications\/2024\/universal@cross-lingual@data@generation@for@low@resource@asr","storage_key":"03.research\/02.publications\/2024\/Universal@Cross-Lingual@Data@Generation@for@Low@Resource@ASR","template":"default","storage_timestamp":1712511400,"markdown":{"":{"default":1712511400}},"checksum":"0106207838d95f46a12c3b180fbdf2ba"},"03.research\/bibtex_helper":{"key":"research\/bibtex_helper","storage_key":"03.research\/bibtex_helper","template":"bibtex_helper","storage_timestamp":1712515162,"markdown":{"":{"bibtex_helper":1712515162}},"checksum":"e92b948e6e020a5bed444c827e179a48"},"03.research\/_03.achievements":{"key":"research\/_03.achievements","storage_key":"03.research\/_03.achievements","template":null,"storage_timestamp":1705239539,"checksum":"3062e60f6ebd90730ead32f1790b8c5e"},"04.members":{"key":"members","storage_key":"04.members","template":"members","storage_timestamp":1709473369,"markdown":{"en":{"members":1709275550},"zh-hans":{"members":1709275563}},"children":{"bing.han":1709458168,"chenda.li":1709458168,"chenyang.le":1709458168,"dongning.yang":1709458168,"haibin.yu":1709458168,"hang.shao":1709458168,"haoxiang.hou":1709458168,"haoyu.wang":1709458168,"jiahong.li":1709458168,"jianze.li":1709458168,"leying.zhang":1709458168,"linfeng.yu":1709458168,"shaoxiong.lin":1709458168,"siyi.zhao":1709458168,"tianteng.gu":1709458168,"tingxiao.zhou":1709458168,"wangyou.zhang":1709458168,"wei.wang":1709458168,"wen.huang":1709458168,"xin.zhou":1709458168,"xun.gong":1709458168,"yanmin.qian":1709458168,"yingpeng.zhang":1709458168,"zhengyang.chen":1709458168},"checksum":"8b06802df1f0149de30e3581617c9228"},"04.members\/bing.han":{"key":"members\/bing.han","storage_key":"04.members\/bing.han","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"db52ce4e967b3586761df0cd85b7422f"},"04.members\/chenda.li":{"key":"members\/chenda.li","storage_key":"04.members\/chenda.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"f4abd3ea74eaa7e7e92ef325894aea9e"},"04.members\/chenyang.le":{"key":"members\/chenyang.le","storage_key":"04.members\/chenyang.le","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"91bc6b6c07f7bc0486086181a13a7577"},"04.members\/dongning.yang":{"key":"members\/dongning.yang","storage_key":"04.members\/dongning.yang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"da3b4770ff947fd7e40b105deecc8398"},"04.members\/haibin.yu":{"key":"members\/haibin.yu","storage_key":"04.members\/haibin.yu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"f6f84c239c53727314b885eb0f00b42a"},"04.members\/hang.shao":{"key":"members\/hang.shao","storage_key":"04.members\/hang.shao","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"502ca7c2efa6340b73e32f95a8963cd1"},"04.members\/haoxiang.hou":{"key":"members\/haoxiang.hou","storage_key":"04.members\/haoxiang.hou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"7633478346b4caec38734fab16abfe8e"},"04.members\/haoyu.wang":{"key":"members\/haoyu.wang","storage_key":"04.members\/haoyu.wang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"83c16c5e415b2f2be97ab8fa57efe9f3"},"04.members\/jiahong.li":{"key":"members\/jiahong.li","storage_key":"04.members\/jiahong.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"77255fe055c7ebe6f59e7447a4c620fb"},"04.members\/jianze.li":{"key":"members\/jianze.li","storage_key":"04.members\/jianze.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"c80394027a19f75e52ccf7a766446d38"},"04.members\/leying.zhang":{"key":"members\/leying.zhang","storage_key":"04.members\/leying.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"dbd5c9c91ee1bfb9a833dc0c9d284470"},"04.members\/linfeng.yu":{"key":"members\/linfeng.yu","storage_key":"04.members\/linfeng.yu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"dd16f081c1d7bdff4efad1d378e28b0d"},"04.members\/shaoxiong.lin":{"key":"members\/shaoxiong.lin","storage_key":"04.members\/shaoxiong.lin","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"3cf4be20f39575883b5a81df82bab99e"},"04.members\/siyi.zhao":{"key":"members\/siyi.zhao","storage_key":"04.members\/siyi.zhao","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"9ceb84cc3d7f3b074bf0067d3059162b"},"04.members\/tianteng.gu":{"key":"members\/tianteng.gu","storage_key":"04.members\/tianteng.gu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"67187bf7a1193fd8aee30e1fc86c8f43"},"04.members\/tingxiao.zhou":{"key":"members\/tingxiao.zhou","storage_key":"04.members\/tingxiao.zhou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"ed63f2f8014780fa0f5ed205c46cea6c"},"04.members\/wangyou.zhang":{"key":"members\/wangyou.zhang","storage_key":"04.members\/wangyou.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"51a28fd58681eda2d0594078902427c3"},"04.members\/wei.wang":{"key":"members\/wei.wang","storage_key":"04.members\/wei.wang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"aa7ca181b6dcddd55cf1fcf6c1d1f4a1"},"04.members\/wen.huang":{"key":"members\/wen.huang","storage_key":"04.members\/wen.huang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"51d168e38a1bd962f19024ddba738e74"},"04.members\/xin.zhou":{"key":"members\/xin.zhou","storage_key":"04.members\/xin.zhou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"528b74a72206a98001ccb374671ee912"},"04.members\/xun.gong":{"key":"members\/xun.gong","storage_key":"04.members\/xun.gong","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"fca850ca2792e41ca340a4dac4952cb1"},"04.members\/yanmin.qian":{"key":"members\/yanmin.qian","storage_key":"04.members\/yanmin.qian","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"333614fff58bd00a07b9ba16d0686829"},"04.members\/yingpeng.zhang":{"key":"members\/yingpeng.zhang","storage_key":"04.members\/yingpeng.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"9f749cd3fae0a4b264e22bb394cdaca3"},"04.members\/zhengyang.chen":{"key":"members\/zhengyang.chen","storage_key":"04.members\/zhengyang.chen","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"fb627e5575f76faf5df1c4615867e2a4"},"05.contact":{"key":"contact","storage_key":"05.contact","template":"contact","storage_timestamp":1712518055,"markdown":{"en":{"contact":1712518042},"zh-hans":{"contact":1712518055}},"checksum":"c0819e79cae62895cd2ded2493f2f367"},"backup":{"key":"backup","storage_key":"backup","template":null,"storage_timestamp":1704688029,"children":{"04.shortcodes":1704590309,"forgot_password":1704590309,"login":1704590309},"checksum":"9545b23e609082764f510dc35f6923b9"},"backup\/04.shortcodes":{"key":"backup\/shortcodes","storage_key":"backup\/04.shortcodes","template":"default","storage_timestamp":1704590309,"markdown":{"":{"default":1704590309}},"checksum":"6d47876968d7a2645ba335a50eca4161"},"backup\/forgot_password":{"key":"backup\/forgot_password","storage_key":"backup\/forgot_password","template":"forgot","storage_timestamp":1704590309,"markdown":{"":{"forgot":1704590309}},"checksum":"cf6acf1c20cb51798ca76f3c1704714e"},"backup\/login":{"key":"backup\/login","storage_key":"backup\/login","template":"login","storage_timestamp":1704590309,"markdown":{"":{"login":1704590309}},"checksum":"d7968627088617f2cf775aad2d00885c"}}}