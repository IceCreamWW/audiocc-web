{"version":"1.5","timestamp":1709472425,"count":126,"index":{"":{"key":"","storage_key":"","template":null,"storage_timestamp":1704688029,"children":{"01.home":1704871816,"02.news":1704665586,"03.research":1704677623,"04.members":1704667918,"05.contact":1704688085,"backup":1704688029},"checksum":"2674c68c90233153549cd964092b2154"},"01.home":{"key":"home","storage_key":"01.home","template":"modular","storage_timestamp":1705239384,"markdown":{"en":{"modular":1705239384},"zh-hans":{"modular":1705239384}},"children":{"01._intro":1705239412,"02._all-stories":1704702097},"checksum":"f067072bc2322642524a926094f9f7e9"},"01.home\/01._intro":{"key":"home\/_intro","storage_key":"01.home\/01._intro","template":"intro","storage_timestamp":1705239412,"markdown":{"en":{"intro":1705239401},"zh-hans":{"intro":1705239274}},"checksum":"be0faf0b089ebb2a86edaed21f9eadbd"},"01.home\/02._all-stories":{"key":"home\/_all-stories","storage_key":"01.home\/02._all-stories","template":"allstories","storage_timestamp":1704702097,"markdown":{"en":{"allstories":1704688149},"zh-hans":{"allstories":1704688139}},"checksum":"cb62d0f6660ad8341393a59802c22a15"},"02.news":{"key":"news","storage_key":"02.news","template":"blog","storage_timestamp":1709471264,"markdown":{"en":{"blog":1704664726},"zh-hans":{"blog":1704665586}},"children":{"GgOVCwU582yLxHd4QCFxBA":1709469241,"MGi5OgI4tmYH7EZOZMSjhA":1709471989,"V2RshWpPwc5rshs0IgaX5Q":1709471996,"welcome-to-grav":1704590309,"yGSGDwGRmxM-ID9SOEWeEg":1709472009},"checksum":"1263918694269468d8d1547119ac158b"},"02.news\/GgOVCwU582yLxHd4QCFxBA":{"key":"news\/ggovcwu582ylxhd4qcfxba","storage_key":"02.news\/GgOVCwU582yLxHd4QCFxBA","template":"item","storage_timestamp":1709469241,"markdown":{"en":{"item":1709460938},"zh-hans":{"item":1709460938}},"checksum":"692fc67d427933824e9f0ac7787c9c2c"},"02.news\/MGi5OgI4tmYH7EZOZMSjhA":{"key":"news\/mgi5ogi4tmyh7ezozmsjha","storage_key":"02.news\/MGi5OgI4tmYH7EZOZMSjhA","template":"item","storage_timestamp":1709471989,"markdown":{"en":{"item":1709469307},"zh-hans":{"item":1709471989}},"checksum":"2f957a4602872e1bc59d15bf2613b9b2"},"02.news\/V2RshWpPwc5rshs0IgaX5Q":{"key":"news\/v2rshwppwc5rshs0igax5q","storage_key":"02.news\/V2RshWpPwc5rshs0IgaX5Q","template":"item","storage_timestamp":1709471996,"markdown":{"en":{"item":1709461034},"zh-hans":{"item":1709471996}},"checksum":"dcc1385761f48774fd19a7c31f2c7093"},"02.news\/welcome-to-grav":{"key":"news\/welcome-to-grav","storage_key":"02.news\/welcome-to-grav","template":"item","storage_timestamp":1704590309,"markdown":{"":{"item":1704590309}},"checksum":"67ff57d54446c292a99970651beb664f"},"02.news\/yGSGDwGRmxM-ID9SOEWeEg":{"key":"news\/ygsgdwgrmxm-id9soeweeg","storage_key":"02.news\/yGSGDwGRmxM-ID9SOEWeEg","template":"item","storage_timestamp":1709472009,"markdown":{"en":{"item":1709460822},"zh-hans":{"item":1709472009}},"checksum":"038c36ba7cec238ccac1a7475d0b99b0"},"03.research":{"key":"research","storage_key":"03.research","template":"default","storage_timestamp":1705239539,"markdown":{"en":{"default":1704667619},"zh-hans":{"default":1704667607}},"children":{"01.fields":1704677693,"02.publications":1709221882,"03.achievements":1705239539,"backup":1704677612},"checksum":"23dc1fe5f9b9c476b5505ee4c4edcf4e"},"03.research\/01.fields":{"key":"research\/fields","storage_key":"03.research\/01.fields","template":"default","storage_timestamp":1704677693,"markdown":{"":{"default":1704677693}},"checksum":"1e88753164e25eed8d74401af704c8a6"},"03.research\/02.publications":{"key":"research\/publications","storage_key":"03.research\/02.publications","template":"publications","storage_timestamp":1709221882,"markdown":{"en":{"publications":1709120034},"zh-hans":{"publications":1709196737}},"children":{"2022":1709026187,"2023":1709026187,"2024":1709026187},"checksum":"324296473c93c21a35da06982ed8d6ce"},"03.research\/02.publications\/2022":{"key":"research\/publications\/2022","storage_key":"03.research\/02.publications\/2022","template":null,"storage_timestamp":1709026187,"children":{"acomprehensivestudyonselfsuperviseddistillationforspeakerrepresentationlearning":1709026187,"attentivefeaturefusionforrobustspeakerverification":1709026187,"buildasrechallengesystemlessonsfromvoxsrcandcnsrc":1709026187,"dfresnetboostingspeakerverificationperformancewithdepthfirstdesign":1709026187,"dualpathembeddinglearningforspeakerverificationwithtripletattention":1709026187,"dualpathmodelingwithmemoryembeddingmodelforcontinuousspeechseparation":1709026187,"endtoenddereverberationbeamformingandspeechrecognitioninacocktailparty":1709026187,"endtoendmultispeakerasrwithindependentvectoranalysis":1709026187,"enrollawareattentivestatisticspoolingfortargetspeakerverification":1709026187,"espnetsespeechenhancementforrobustspeechrecognitiontranslationandunderstanding":1709026187,"exploringeffectivedatautilizationforlowresourcespeechrecognition":1709026187,"improvingspeechseparationwithknowledgedistilledfromselfsupervisedpretrainedmodels":1709026187,"knowledgetransferanddistillationfromautoregressivetononautoregessivespeechrecognition":1709026187,"knowledgetransferanddistillationfromautoregressivetononautoregressivespeechrecognition":1709026187,"largescaleselfsupervisedspeechrepresentationlearningforautomaticspeakerverification":1709026187,"layerwisefastadaptationforendtoendmultiaccentspeechrecognition":1709026187,"localinformationmodelingwithselfattentionforspeakerverification":1709026187,"longfntlongformspeechrecognitionwithfactorizedneuraltransducer":1709026187,"medicaldifficultairwaydetectionusingspeechtechnology":1709026187,"mlpsvnetamultilayerperceptronsbasednetworkforspeakerverification":1709026187,"msdwildmultimodalspeakerdiarizationdatasetinthewild":1709026187,"optimizingalignmentofspeechandlanguagelatentspacesforendtoendspeechrecognitionandunderstanding":1709026187,"optimizingdatausageforlowresourcespeechrecognition":1709026187,"punctuationpredictionforstreamingondevicespeechrecognition":1709026187,"selfknowledgedistillationviafeatureenhancementforspeakerverification":1709026187,"selfsupervisedspeakerverificationusingdynamiclossgateandlabelcorrection":1709026187,"separatinglongformspeechwithgroupwisepermutationinvarianttraining":1709026187,"sjtuaispeechsystemforvoxcelebspeakerrecognitionchallenge":1709026187,"skimskippingmemorylstmforlowlatencyrealtimecontinuousspeechseparation":1709026187,"speakingstylecompensationonsyntheticaudioforrobustkeywordspotting":1709026187,"summaryontheicasspmultichannelmultipartymeetingtranscriptiongrandchallenge":1709026187,"textinformedknowledgedistillationforrobustspeechenhancementandrecognition":1709026187,"theconversationalshortphrasespeakerdiarizationcssdtaskdatasetevaluationmetricandbaselines":1709026187,"thesjtusystemformultimodalinformationbasedspeechprocessingchallenge":1709026187,"thesjtusystemforshortdurationspeakerverificationchallenge":1709026187,"thesjtuxlancelabsystemforcnsrc":1709026187,"thexlancespeakerdiarizationsystemfortheconversationalshortphrasespeakerdiarizationchallenge":1709026187,"timedomainaudiovisualspeechseparationonlowqualityvideos":1709026187,"wavlmlargescaleselfsupervisedpretrainingforfullstackspeechprocessing":1709026187,"wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit":1709026187},"checksum":"e157dc0abe989726455b536a769a8dda"},"03.research\/02.publications\/2022\/acomprehensivestudyonselfsuperviseddistillationforspeakerrepresentationlearning":{"key":"research\/publications\/2022\/acomprehensivestudyonselfsuperviseddistillationforspeakerrepresentationlearning","storage_key":"03.research\/02.publications\/2022\/acomprehensivestudyonselfsuperviseddistillationforspeakerrepresentationlearning","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"95e6e78f709a5cda4070e6520e9463ed"},"03.research\/02.publications\/2022\/attentivefeaturefusionforrobustspeakerverification":{"key":"research\/publications\/2022\/attentivefeaturefusionforrobustspeakerverification","storage_key":"03.research\/02.publications\/2022\/attentivefeaturefusionforrobustspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"59f37cf02b373e10935a01affe296ed0"},"03.research\/02.publications\/2022\/buildasrechallengesystemlessonsfromvoxsrcandcnsrc":{"key":"research\/publications\/2022\/buildasrechallengesystemlessonsfromvoxsrcandcnsrc","storage_key":"03.research\/02.publications\/2022\/buildasrechallengesystemlessonsfromvoxsrcandcnsrc","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"c8e7992c999b27c85dc82d28ddaacfbc"},"03.research\/02.publications\/2022\/dfresnetboostingspeakerverificationperformancewithdepthfirstdesign":{"key":"research\/publications\/2022\/dfresnetboostingspeakerverificationperformancewithdepthfirstdesign","storage_key":"03.research\/02.publications\/2022\/dfresnetboostingspeakerverificationperformancewithdepthfirstdesign","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"6e806f5d7bda67b9f3c220ba06237a7c"},"03.research\/02.publications\/2022\/dualpathembeddinglearningforspeakerverificationwithtripletattention":{"key":"research\/publications\/2022\/dualpathembeddinglearningforspeakerverificationwithtripletattention","storage_key":"03.research\/02.publications\/2022\/dualpathembeddinglearningforspeakerverificationwithtripletattention","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"9a11db1a6edd499b0d0f6298906a553f"},"03.research\/02.publications\/2022\/dualpathmodelingwithmemoryembeddingmodelforcontinuousspeechseparation":{"key":"research\/publications\/2022\/dualpathmodelingwithmemoryembeddingmodelforcontinuousspeechseparation","storage_key":"03.research\/02.publications\/2022\/dualpathmodelingwithmemoryembeddingmodelforcontinuousspeechseparation","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"51a012c56bcf0c3203d2fd514e7fecd9"},"03.research\/02.publications\/2022\/endtoenddereverberationbeamformingandspeechrecognitioninacocktailparty":{"key":"research\/publications\/2022\/endtoenddereverberationbeamformingandspeechrecognitioninacocktailparty","storage_key":"03.research\/02.publications\/2022\/endtoenddereverberationbeamformingandspeechrecognitioninacocktailparty","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"cde362621b1cac9426e8e07b5b2668b7"},"03.research\/02.publications\/2022\/endtoendmultispeakerasrwithindependentvectoranalysis":{"key":"research\/publications\/2022\/endtoendmultispeakerasrwithindependentvectoranalysis","storage_key":"03.research\/02.publications\/2022\/endtoendmultispeakerasrwithindependentvectoranalysis","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"7dc482f1558cfbab352ee872f4f5ca83"},"03.research\/02.publications\/2022\/enrollawareattentivestatisticspoolingfortargetspeakerverification":{"key":"research\/publications\/2022\/enrollawareattentivestatisticspoolingfortargetspeakerverification","storage_key":"03.research\/02.publications\/2022\/enrollawareattentivestatisticspoolingfortargetspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"f147f74f60234da4e1f104e8f13e292e"},"03.research\/02.publications\/2022\/espnetsespeechenhancementforrobustspeechrecognitiontranslationandunderstanding":{"key":"research\/publications\/2022\/espnetsespeechenhancementforrobustspeechrecognitiontranslationandunderstanding","storage_key":"03.research\/02.publications\/2022\/espnetsespeechenhancementforrobustspeechrecognitiontranslationandunderstanding","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"eb5b3d68df94935a68abe5b5bf269af0"},"03.research\/02.publications\/2022\/exploringeffectivedatautilizationforlowresourcespeechrecognition":{"key":"research\/publications\/2022\/exploringeffectivedatautilizationforlowresourcespeechrecognition","storage_key":"03.research\/02.publications\/2022\/exploringeffectivedatautilizationforlowresourcespeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"3c246d196cdfc64b099fc7757d8ad38f"},"03.research\/02.publications\/2022\/improvingspeechseparationwithknowledgedistilledfromselfsupervisedpretrainedmodels":{"key":"research\/publications\/2022\/improvingspeechseparationwithknowledgedistilledfromselfsupervisedpretrainedmodels","storage_key":"03.research\/02.publications\/2022\/improvingspeechseparationwithknowledgedistilledfromselfsupervisedpretrainedmodels","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"7d5b0527f36b05e85d4f52c57ed916b4"},"03.research\/02.publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregessivespeechrecognition":{"key":"research\/publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregessivespeechrecognition","storage_key":"03.research\/02.publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregessivespeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"8bb2f57d7727a5cd880a042bc4b56af1"},"03.research\/02.publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregressivespeechrecognition":{"key":"research\/publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregressivespeechrecognition","storage_key":"03.research\/02.publications\/2022\/knowledgetransferanddistillationfromautoregressivetononautoregressivespeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"a6be969e4c28ab2b15897cfb735f6222"},"03.research\/02.publications\/2022\/largescaleselfsupervisedspeechrepresentationlearningforautomaticspeakerverification":{"key":"research\/publications\/2022\/largescaleselfsupervisedspeechrepresentationlearningforautomaticspeakerverification","storage_key":"03.research\/02.publications\/2022\/largescaleselfsupervisedspeechrepresentationlearningforautomaticspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"b0891a5f812183435cda8c31c66057f4"},"03.research\/02.publications\/2022\/layerwisefastadaptationforendtoendmultiaccentspeechrecognition":{"key":"research\/publications\/2022\/layerwisefastadaptationforendtoendmultiaccentspeechrecognition","storage_key":"03.research\/02.publications\/2022\/layerwisefastadaptationforendtoendmultiaccentspeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"b50ebdc85799fe19d4955131dafae5ec"},"03.research\/02.publications\/2022\/localinformationmodelingwithselfattentionforspeakerverification":{"key":"research\/publications\/2022\/localinformationmodelingwithselfattentionforspeakerverification","storage_key":"03.research\/02.publications\/2022\/localinformationmodelingwithselfattentionforspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"4d9d602eda8c5858a448c875614bd91b"},"03.research\/02.publications\/2022\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer":{"key":"research\/publications\/2022\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer","storage_key":"03.research\/02.publications\/2022\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"feef863978a194d890818afdd8c35ad1"},"03.research\/02.publications\/2022\/medicaldifficultairwaydetectionusingspeechtechnology":{"key":"research\/publications\/2022\/medicaldifficultairwaydetectionusingspeechtechnology","storage_key":"03.research\/02.publications\/2022\/medicaldifficultairwaydetectionusingspeechtechnology","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"53f7b5e44d9b8f0e004edc8662584234"},"03.research\/02.publications\/2022\/mlpsvnetamultilayerperceptronsbasednetworkforspeakerverification":{"key":"research\/publications\/2022\/mlpsvnetamultilayerperceptronsbasednetworkforspeakerverification","storage_key":"03.research\/02.publications\/2022\/mlpsvnetamultilayerperceptronsbasednetworkforspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"9fd2699f6b9d5d6c568fd2b235313315"},"03.research\/02.publications\/2022\/msdwildmultimodalspeakerdiarizationdatasetinthewild":{"key":"research\/publications\/2022\/msdwildmultimodalspeakerdiarizationdatasetinthewild","storage_key":"03.research\/02.publications\/2022\/msdwildmultimodalspeakerdiarizationdatasetinthewild","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"c1848a9500ca3f1cb507fc1d53ad9ec0"},"03.research\/02.publications\/2022\/optimizingalignmentofspeechandlanguagelatentspacesforendtoendspeechrecognitionandunderstanding":{"key":"research\/publications\/2022\/optimizingalignmentofspeechandlanguagelatentspacesforendtoendspeechrecognitionandunderstanding","storage_key":"03.research\/02.publications\/2022\/optimizingalignmentofspeechandlanguagelatentspacesforendtoendspeechrecognitionandunderstanding","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"a1ec3ba04c61ffe151f3a30dbc7ac8e3"},"03.research\/02.publications\/2022\/optimizingdatausageforlowresourcespeechrecognition":{"key":"research\/publications\/2022\/optimizingdatausageforlowresourcespeechrecognition","storage_key":"03.research\/02.publications\/2022\/optimizingdatausageforlowresourcespeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"23318cebeb160515d0f84b042db8f8d9"},"03.research\/02.publications\/2022\/punctuationpredictionforstreamingondevicespeechrecognition":{"key":"research\/publications\/2022\/punctuationpredictionforstreamingondevicespeechrecognition","storage_key":"03.research\/02.publications\/2022\/punctuationpredictionforstreamingondevicespeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"9669f48215e3e95e8480af03c06cb3fd"},"03.research\/02.publications\/2022\/selfknowledgedistillationviafeatureenhancementforspeakerverification":{"key":"research\/publications\/2022\/selfknowledgedistillationviafeatureenhancementforspeakerverification","storage_key":"03.research\/02.publications\/2022\/selfknowledgedistillationviafeatureenhancementforspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"fc2ba1e7fd71036168c58a6d6b919bb4"},"03.research\/02.publications\/2022\/selfsupervisedspeakerverificationusingdynamiclossgateandlabelcorrection":{"key":"research\/publications\/2022\/selfsupervisedspeakerverificationusingdynamiclossgateandlabelcorrection","storage_key":"03.research\/02.publications\/2022\/selfsupervisedspeakerverificationusingdynamiclossgateandlabelcorrection","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"b3b973603b976b31d85ab1de45962a69"},"03.research\/02.publications\/2022\/separatinglongformspeechwithgroupwisepermutationinvarianttraining":{"key":"research\/publications\/2022\/separatinglongformspeechwithgroupwisepermutationinvarianttraining","storage_key":"03.research\/02.publications\/2022\/separatinglongformspeechwithgroupwisepermutationinvarianttraining","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"6f9eadd4163fa2b626690d4e1de9c2dd"},"03.research\/02.publications\/2022\/sjtuaispeechsystemforvoxcelebspeakerrecognitionchallenge":{"key":"research\/publications\/2022\/sjtuaispeechsystemforvoxcelebspeakerrecognitionchallenge","storage_key":"03.research\/02.publications\/2022\/sjtuaispeechsystemforvoxcelebspeakerrecognitionchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"cc7d7f5d5e68a0bddc41e5afb5692f01"},"03.research\/02.publications\/2022\/skimskippingmemorylstmforlowlatencyrealtimecontinuousspeechseparation":{"key":"research\/publications\/2022\/skimskippingmemorylstmforlowlatencyrealtimecontinuousspeechseparation","storage_key":"03.research\/02.publications\/2022\/skimskippingmemorylstmforlowlatencyrealtimecontinuousspeechseparation","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"eaf4e7d5110a4d34195afb996351f593"},"03.research\/02.publications\/2022\/speakingstylecompensationonsyntheticaudioforrobustkeywordspotting":{"key":"research\/publications\/2022\/speakingstylecompensationonsyntheticaudioforrobustkeywordspotting","storage_key":"03.research\/02.publications\/2022\/speakingstylecompensationonsyntheticaudioforrobustkeywordspotting","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"6c76f524d573a4a24f2785208e77311e"},"03.research\/02.publications\/2022\/summaryontheicasspmultichannelmultipartymeetingtranscriptiongrandchallenge":{"key":"research\/publications\/2022\/summaryontheicasspmultichannelmultipartymeetingtranscriptiongrandchallenge","storage_key":"03.research\/02.publications\/2022\/summaryontheicasspmultichannelmultipartymeetingtranscriptiongrandchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"7d459b0db177838283590e696786e1fb"},"03.research\/02.publications\/2022\/textinformedknowledgedistillationforrobustspeechenhancementandrecognition":{"key":"research\/publications\/2022\/textinformedknowledgedistillationforrobustspeechenhancementandrecognition","storage_key":"03.research\/02.publications\/2022\/textinformedknowledgedistillationforrobustspeechenhancementandrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"5c8353bd71b4e26e5f227941c4ac74f6"},"03.research\/02.publications\/2022\/theconversationalshortphrasespeakerdiarizationcssdtaskdatasetevaluationmetricandbaselines":{"key":"research\/publications\/2022\/theconversationalshortphrasespeakerdiarizationcssdtaskdatasetevaluationmetricandbaselines","storage_key":"03.research\/02.publications\/2022\/theconversationalshortphrasespeakerdiarizationcssdtaskdatasetevaluationmetricandbaselines","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"8aee6f113ca36d85bc2128aa5e840bd6"},"03.research\/02.publications\/2022\/thesjtusystemformultimodalinformationbasedspeechprocessingchallenge":{"key":"research\/publications\/2022\/thesjtusystemformultimodalinformationbasedspeechprocessingchallenge","storage_key":"03.research\/02.publications\/2022\/thesjtusystemformultimodalinformationbasedspeechprocessingchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"45505bb5efd6d46ef8a1c882b713adbf"},"03.research\/02.publications\/2022\/thesjtusystemforshortdurationspeakerverificationchallenge":{"key":"research\/publications\/2022\/thesjtusystemforshortdurationspeakerverificationchallenge","storage_key":"03.research\/02.publications\/2022\/thesjtusystemforshortdurationspeakerverificationchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"bdba87d4c762830a3f29b293800aa009"},"03.research\/02.publications\/2022\/thesjtuxlancelabsystemforcnsrc":{"key":"research\/publications\/2022\/thesjtuxlancelabsystemforcnsrc","storage_key":"03.research\/02.publications\/2022\/thesjtuxlancelabsystemforcnsrc","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"43aa09ad857110cfeb8b61483253cbb0"},"03.research\/02.publications\/2022\/thexlancespeakerdiarizationsystemfortheconversationalshortphrasespeakerdiarizationchallenge":{"key":"research\/publications\/2022\/thexlancespeakerdiarizationsystemfortheconversationalshortphrasespeakerdiarizationchallenge","storage_key":"03.research\/02.publications\/2022\/thexlancespeakerdiarizationsystemfortheconversationalshortphrasespeakerdiarizationchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"973e87329fc8767f79e5250d9c6d627f"},"03.research\/02.publications\/2022\/timedomainaudiovisualspeechseparationonlowqualityvideos":{"key":"research\/publications\/2022\/timedomainaudiovisualspeechseparationonlowqualityvideos","storage_key":"03.research\/02.publications\/2022\/timedomainaudiovisualspeechseparationonlowqualityvideos","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"89b932c8207afafa87eafa6a9cf0501d"},"03.research\/02.publications\/2022\/wavlmlargescaleselfsupervisedpretrainingforfullstackspeechprocessing":{"key":"research\/publications\/2022\/wavlmlargescaleselfsupervisedpretrainingforfullstackspeechprocessing","storage_key":"03.research\/02.publications\/2022\/wavlmlargescaleselfsupervisedpretrainingforfullstackspeechprocessing","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"7da1f4cac831db499468f2c1c3873df7"},"03.research\/02.publications\/2022\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit":{"key":"research\/publications\/2022\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit","storage_key":"03.research\/02.publications\/2022\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"f2177b46859d973ce644f46926376255"},"03.research\/02.publications\/2023":{"key":"research\/publications\/2023","storage_key":"03.research\/02.publications\/2023","template":null,"storage_timestamp":1709026187,"children":{"adaptingmultilingualasrmodelsforhandlingmultipletalkers":1709026187,"adaptivelargemarginfinetuningforrobustspeakerverification":1709026187,"attentionbasedencoderdecoderendtoendneuraldiarizationwithembeddingenhancer":1709026187,"attentionbasedencoderdecodernetworkforendtoendneuralspeakerdiarizationwithtargetspeakerattractor":1709026187,"codeswitchingtextgenerationandinjectioninmandarinenglishasr":1709026187,"depthfirstneuralarchitecturewithattentivefeaturefusionforefficientspeakerverification":1709026187,"diffusionconditionalexpectationmodelforefficientandrobusttargetspeechextraction":1709026187,"exploringbinaryclassificationlossforspeakerverification":1709026187,"exploringtheintegrationofspeechseparationandrecognitionwithselfsupervisedlearningrepresentation":1709026187,"factorizedaedfactorizedattentionbasedencoderdecoderfortextonlydomainadaptiveasr":1709026187,"fathubertfrontendadaptivetrainingofhiddenunitbertfordistortioninvariantrobustspeechrecognition":1709026187,"hubertaggaggregatedrepresentationdistillationofhiddenunitbertforrobustspeechrecognition":1709026187,"improvingdinobasedselfsupervisedspeakerverificationwithprogressiveclusterawaretraining":1709026187,"instructmeaninstructionguidedmusiceditandremixframeworkwithlatentdiffusionmodels":1709026187,"jointdiscriminatorandtransferbasedfastdomainadaptationforendtoendspeechrecognition":1709026187,"leveraginginthewilddataforeffectiveselfsupervisedpretraininginspeakerrecognition":1709026187,"lightweightvisualvoiceneuralnetworkquantizationonaudiovisualspeechseparation":1709026187,"longfntlongformspeechrecognitionwithfactorizedneuraltransducer":1709026187,"lowbitneuralnetworkquantizationforspeakerverification":1709026187,"multispeakerendtoendmultimodalspeakerdiarizationsystemforthemispchallenge":1709026187,"oneshotsensitivityawaremixedsparsitypruningforlargelanguagemodels":1709026187,"predictiveskimcontrastivepredictivecodingforlowlatencyonlinespeechseparation":1709026187,"robustaudiovisualasrwithunifiedcrossmodalattention":1709026187,"selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification":1709026187,"softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessing":1709026187,"softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessingespnetvversion":1709026187,"targetsoundextractionwithvariablecrossmodalityclues":1709026187,"thesecondmultichannelmultipartymeetingtranscriptionchallengemmetabenchmarkforspeakerattributedasr":1709026187,"towarduniversalspeechenhancementfordiverseinputconditions":1709026187,"useduniversalspeakerextractionanddiarization":1709026187,"weaklysupervisedspeechpretrainingacasestudyontargetspeechrecognition":1709026187,"wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit":1709026187,"whisperkdqalightweightwhisperviaguidedknowledgedistillationandquantizationforefficientasr":1709026187},"checksum":"a2c50d71256394a8c3a6e699f5667e99"},"03.research\/02.publications\/2023\/adaptingmultilingualasrmodelsforhandlingmultipletalkers":{"key":"research\/publications\/2023\/adaptingmultilingualasrmodelsforhandlingmultipletalkers","storage_key":"03.research\/02.publications\/2023\/adaptingmultilingualasrmodelsforhandlingmultipletalkers","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"3b60bb213c29b4aa5f00d936f42a9c89"},"03.research\/02.publications\/2023\/adaptivelargemarginfinetuningforrobustspeakerverification":{"key":"research\/publications\/2023\/adaptivelargemarginfinetuningforrobustspeakerverification","storage_key":"03.research\/02.publications\/2023\/adaptivelargemarginfinetuningforrobustspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"95fb909037096743403a690891182502"},"03.research\/02.publications\/2023\/attentionbasedencoderdecoderendtoendneuraldiarizationwithembeddingenhancer":{"key":"research\/publications\/2023\/attentionbasedencoderdecoderendtoendneuraldiarizationwithembeddingenhancer","storage_key":"03.research\/02.publications\/2023\/attentionbasedencoderdecoderendtoendneuraldiarizationwithembeddingenhancer","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"739f7464be7eec131cc267adffec5bd9"},"03.research\/02.publications\/2023\/attentionbasedencoderdecodernetworkforendtoendneuralspeakerdiarizationwithtargetspeakerattractor":{"key":"research\/publications\/2023\/attentionbasedencoderdecodernetworkforendtoendneuralspeakerdiarizationwithtargetspeakerattractor","storage_key":"03.research\/02.publications\/2023\/attentionbasedencoderdecodernetworkforendtoendneuralspeakerdiarizationwithtargetspeakerattractor","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"d6e381ac7b2eaa2d181b5cfd6cb25498"},"03.research\/02.publications\/2023\/codeswitchingtextgenerationandinjectioninmandarinenglishasr":{"key":"research\/publications\/2023\/codeswitchingtextgenerationandinjectioninmandarinenglishasr","storage_key":"03.research\/02.publications\/2023\/codeswitchingtextgenerationandinjectioninmandarinenglishasr","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"f2cda72569282f3e65bcbecf4aa9a95f"},"03.research\/02.publications\/2023\/depthfirstneuralarchitecturewithattentivefeaturefusionforefficientspeakerverification":{"key":"research\/publications\/2023\/depthfirstneuralarchitecturewithattentivefeaturefusionforefficientspeakerverification","storage_key":"03.research\/02.publications\/2023\/depthfirstneuralarchitecturewithattentivefeaturefusionforefficientspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"24563d34d08804e6b99137c191896e59"},"03.research\/02.publications\/2023\/diffusionconditionalexpectationmodelforefficientandrobusttargetspeechextraction":{"key":"research\/publications\/2023\/diffusionconditionalexpectationmodelforefficientandrobusttargetspeechextraction","storage_key":"03.research\/02.publications\/2023\/diffusionconditionalexpectationmodelforefficientandrobusttargetspeechextraction","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"9be2384e9831f99a9aa9e77336308649"},"03.research\/02.publications\/2023\/exploringbinaryclassificationlossforspeakerverification":{"key":"research\/publications\/2023\/exploringbinaryclassificationlossforspeakerverification","storage_key":"03.research\/02.publications\/2023\/exploringbinaryclassificationlossforspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"484b29a3675591b0e59ac5fe7b5ecf7c"},"03.research\/02.publications\/2023\/exploringtheintegrationofspeechseparationandrecognitionwithselfsupervisedlearningrepresentation":{"key":"research\/publications\/2023\/exploringtheintegrationofspeechseparationandrecognitionwithselfsupervisedlearningrepresentation","storage_key":"03.research\/02.publications\/2023\/exploringtheintegrationofspeechseparationandrecognitionwithselfsupervisedlearningrepresentation","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"3fc2f3570090d928cabb52fa5ba98de2"},"03.research\/02.publications\/2023\/factorizedaedfactorizedattentionbasedencoderdecoderfortextonlydomainadaptiveasr":{"key":"research\/publications\/2023\/factorizedaedfactorizedattentionbasedencoderdecoderfortextonlydomainadaptiveasr","storage_key":"03.research\/02.publications\/2023\/factorizedaedfactorizedattentionbasedencoderdecoderfortextonlydomainadaptiveasr","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"eddf9b148653fc3d6760bc33d025b4aa"},"03.research\/02.publications\/2023\/fathubertfrontendadaptivetrainingofhiddenunitbertfordistortioninvariantrobustspeechrecognition":{"key":"research\/publications\/2023\/fathubertfrontendadaptivetrainingofhiddenunitbertfordistortioninvariantrobustspeechrecognition","storage_key":"03.research\/02.publications\/2023\/fathubertfrontendadaptivetrainingofhiddenunitbertfordistortioninvariantrobustspeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"79038be1415bbbafabb45a4628e40d62"},"03.research\/02.publications\/2023\/hubertaggaggregatedrepresentationdistillationofhiddenunitbertforrobustspeechrecognition":{"key":"research\/publications\/2023\/hubertaggaggregatedrepresentationdistillationofhiddenunitbertforrobustspeechrecognition","storage_key":"03.research\/02.publications\/2023\/hubertaggaggregatedrepresentationdistillationofhiddenunitbertforrobustspeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"8c9a5328d45561f186306cd5d3158c3d"},"03.research\/02.publications\/2023\/improvingdinobasedselfsupervisedspeakerverificationwithprogressiveclusterawaretraining":{"key":"research\/publications\/2023\/improvingdinobasedselfsupervisedspeakerverificationwithprogressiveclusterawaretraining","storage_key":"03.research\/02.publications\/2023\/improvingdinobasedselfsupervisedspeakerverificationwithprogressiveclusterawaretraining","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"80513ef73ffb414710f0d9c808096497"},"03.research\/02.publications\/2023\/instructmeaninstructionguidedmusiceditandremixframeworkwithlatentdiffusionmodels":{"key":"research\/publications\/2023\/instructmeaninstructionguidedmusiceditandremixframeworkwithlatentdiffusionmodels","storage_key":"03.research\/02.publications\/2023\/instructmeaninstructionguidedmusiceditandremixframeworkwithlatentdiffusionmodels","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"6adaecc30b4da453564dfee6f121a4a5"},"03.research\/02.publications\/2023\/jointdiscriminatorandtransferbasedfastdomainadaptationforendtoendspeechrecognition":{"key":"research\/publications\/2023\/jointdiscriminatorandtransferbasedfastdomainadaptationforendtoendspeechrecognition","storage_key":"03.research\/02.publications\/2023\/jointdiscriminatorandtransferbasedfastdomainadaptationforendtoendspeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"f24c5ad9a215bf24c00fb960e3e34d44"},"03.research\/02.publications\/2023\/leveraginginthewilddataforeffectiveselfsupervisedpretraininginspeakerrecognition":{"key":"research\/publications\/2023\/leveraginginthewilddataforeffectiveselfsupervisedpretraininginspeakerrecognition","storage_key":"03.research\/02.publications\/2023\/leveraginginthewilddataforeffectiveselfsupervisedpretraininginspeakerrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"30f35312d390ac55adf2c2826cc610e2"},"03.research\/02.publications\/2023\/lightweightvisualvoiceneuralnetworkquantizationonaudiovisualspeechseparation":{"key":"research\/publications\/2023\/lightweightvisualvoiceneuralnetworkquantizationonaudiovisualspeechseparation","storage_key":"03.research\/02.publications\/2023\/lightweightvisualvoiceneuralnetworkquantizationonaudiovisualspeechseparation","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"63063313c64ce8bcaafdda58741509d0"},"03.research\/02.publications\/2023\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer":{"key":"research\/publications\/2023\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer","storage_key":"03.research\/02.publications\/2023\/longfntlongformspeechrecognitionwithfactorizedneuraltransducer","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"ba7230bf94bf813d2b8c4d78ac06de0b"},"03.research\/02.publications\/2023\/lowbitneuralnetworkquantizationforspeakerverification":{"key":"research\/publications\/2023\/lowbitneuralnetworkquantizationforspeakerverification","storage_key":"03.research\/02.publications\/2023\/lowbitneuralnetworkquantizationforspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"ad5fdd369d65b549c38d14f1eec26491"},"03.research\/02.publications\/2023\/multispeakerendtoendmultimodalspeakerdiarizationsystemforthemispchallenge":{"key":"research\/publications\/2023\/multispeakerendtoendmultimodalspeakerdiarizationsystemforthemispchallenge","storage_key":"03.research\/02.publications\/2023\/multispeakerendtoendmultimodalspeakerdiarizationsystemforthemispchallenge","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"10201bf0f9bef203ac39577524339ea7"},"03.research\/02.publications\/2023\/oneshotsensitivityawaremixedsparsitypruningforlargelanguagemodels":{"key":"research\/publications\/2023\/oneshotsensitivityawaremixedsparsitypruningforlargelanguagemodels","storage_key":"03.research\/02.publications\/2023\/oneshotsensitivityawaremixedsparsitypruningforlargelanguagemodels","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"99e570b2e6ac908670ce0a216c59f536"},"03.research\/02.publications\/2023\/predictiveskimcontrastivepredictivecodingforlowlatencyonlinespeechseparation":{"key":"research\/publications\/2023\/predictiveskimcontrastivepredictivecodingforlowlatencyonlinespeechseparation","storage_key":"03.research\/02.publications\/2023\/predictiveskimcontrastivepredictivecodingforlowlatencyonlinespeechseparation","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"ca250c14bcf8c9cb42404e0bbff92688"},"03.research\/02.publications\/2023\/robustaudiovisualasrwithunifiedcrossmodalattention":{"key":"research\/publications\/2023\/robustaudiovisualasrwithunifiedcrossmodalattention","storage_key":"03.research\/02.publications\/2023\/robustaudiovisualasrwithunifiedcrossmodalattention","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"70ff3806443602bbf95050679d2bddd5"},"03.research\/02.publications\/2023\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification":{"key":"research\/publications\/2023\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification","storage_key":"03.research\/02.publications\/2023\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"241403dbf1a7ed39db56e55124622d92"},"03.research\/02.publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessing":{"key":"research\/publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessing","storage_key":"03.research\/02.publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessing","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"ec5347f5ccc948a4f0f176a0092ac3e8"},"03.research\/02.publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessingespnetvversion":{"key":"research\/publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessingespnetvversion","storage_key":"03.research\/02.publications\/2023\/softwaredesignanduserinterfaceofespnetsespeechenhancementforrobustspeechprocessingespnetvversion","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"ef743881b893d1abb857810c799d2fad"},"03.research\/02.publications\/2023\/targetsoundextractionwithvariablecrossmodalityclues":{"key":"research\/publications\/2023\/targetsoundextractionwithvariablecrossmodalityclues","storage_key":"03.research\/02.publications\/2023\/targetsoundextractionwithvariablecrossmodalityclues","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"380d9d1ad979a984dd6b3fdcf584082e"},"03.research\/02.publications\/2023\/thesecondmultichannelmultipartymeetingtranscriptionchallengemmetabenchmarkforspeakerattributedasr":{"key":"research\/publications\/2023\/thesecondmultichannelmultipartymeetingtranscriptionchallengemmetabenchmarkforspeakerattributedasr","storage_key":"03.research\/02.publications\/2023\/thesecondmultichannelmultipartymeetingtranscriptionchallengemmetabenchmarkforspeakerattributedasr","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"be84f15d62a3dc45f1f9035316b3851a"},"03.research\/02.publications\/2023\/towarduniversalspeechenhancementfordiverseinputconditions":{"key":"research\/publications\/2023\/towarduniversalspeechenhancementfordiverseinputconditions","storage_key":"03.research\/02.publications\/2023\/towarduniversalspeechenhancementfordiverseinputconditions","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"2ae306ef3186c1a217217147b7d862e5"},"03.research\/02.publications\/2023\/useduniversalspeakerextractionanddiarization":{"key":"research\/publications\/2023\/useduniversalspeakerextractionanddiarization","storage_key":"03.research\/02.publications\/2023\/useduniversalspeakerextractionanddiarization","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"cc2580d36543af81c5a8547eecacaafb"},"03.research\/02.publications\/2023\/weaklysupervisedspeechpretrainingacasestudyontargetspeechrecognition":{"key":"research\/publications\/2023\/weaklysupervisedspeechpretrainingacasestudyontargetspeechrecognition","storage_key":"03.research\/02.publications\/2023\/weaklysupervisedspeechpretrainingacasestudyontargetspeechrecognition","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"9e03a6d2135638ef1d26657c262093fd"},"03.research\/02.publications\/2023\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit":{"key":"research\/publications\/2023\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit","storage_key":"03.research\/02.publications\/2023\/wespeakeraresearchandproductionorientedspeakerembeddinglearningtoolkit","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"805b60763e638dd9871a0e37d3017722"},"03.research\/02.publications\/2023\/whisperkdqalightweightwhisperviaguidedknowledgedistillationandquantizationforefficientasr":{"key":"research\/publications\/2023\/whisperkdqalightweightwhisperviaguidedknowledgedistillationandquantizationforefficientasr","storage_key":"03.research\/02.publications\/2023\/whisperkdqalightweightwhisperviaguidedknowledgedistillationandquantizationforefficientasr","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"a78d9db279952e480a9a99685669af48"},"03.research\/02.publications\/2024":{"key":"research\/publications\/2024","storage_key":"03.research\/02.publications\/2024","template":null,"storage_timestamp":1709026187,"children":{"improvingdesignofinputconditioninvariantspeechenhancement":1709026187,"selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification":1709026187,"universalcrosslingualdatagenerationforlowresourceasr":1709026187},"checksum":"cd6df91b4ae9c65190ec01032bc7eae7"},"03.research\/02.publications\/2024\/improvingdesignofinputconditioninvariantspeechenhancement":{"key":"research\/publications\/2024\/improvingdesignofinputconditioninvariantspeechenhancement","storage_key":"03.research\/02.publications\/2024\/improvingdesignofinputconditioninvariantspeechenhancement","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"b128ef9edacdccb803d6b91c3e0ee240"},"03.research\/02.publications\/2024\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification":{"key":"research\/publications\/2024\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification","storage_key":"03.research\/02.publications\/2024\/selfsupervisedlearningwithclusterawaredinoforhighperformancerobustspeakerverification","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"a31dad4d5988202a1d1ec535eceb36b3"},"03.research\/02.publications\/2024\/universalcrosslingualdatagenerationforlowresourceasr":{"key":"research\/publications\/2024\/universalcrosslingualdatagenerationforlowresourceasr","storage_key":"03.research\/02.publications\/2024\/universalcrosslingualdatagenerationforlowresourceasr","template":"default","storage_timestamp":1709026187,"markdown":{"":{"default":1709026187}},"checksum":"b1fb2b7b5b8f1d66bd2138b3906990d3"},"03.research\/03.achievements":{"key":"research\/achievements","storage_key":"03.research\/03.achievements","template":null,"storage_timestamp":1705239539,"checksum":"bbd24482876acbe96077e5ee5a9d79cb"},"03.research\/backup":{"key":"research\/backup","storage_key":"03.research\/backup","template":null,"storage_timestamp":1704677612,"children":{"01.typography":1704590309,"02.readme":1704590309},"checksum":"4ccad85c0cb05780b9937b72cb1edb73"},"03.research\/backup\/01.typography":{"key":"research\/backup\/typography","storage_key":"03.research\/backup\/01.typography","template":"default","storage_timestamp":1704590309,"markdown":{"":{"default":1704590309}},"checksum":"48f0c481655f8272cbb8de39341e0a21"},"03.research\/backup\/02.readme":{"key":"research\/backup\/readme","storage_key":"03.research\/backup\/02.readme","template":"default","storage_timestamp":1704590309,"markdown":{"":{"default":1704590309}},"checksum":"764996d3e837ce3558f80bf32d50ecd6"},"04.members":{"key":"members","storage_key":"04.members","template":"members","storage_timestamp":1709458260,"markdown":{"en":{"members":1709275550},"zh-hans":{"members":1709275563}},"children":{"bing.han":1709458168,"chenda.li":1709458168,"chenyang.le":1709458168,"dongning.yang":1709458168,"haibin.yu":1709458168,"hang.shao":1709458168,"haoxiang.hou":1709458168,"haoyu.wang":1709458168,"jiahong.li":1709458168,"jianze.li":1709458168,"leying.zhang":1709458168,"linfeng.yu":1709458168,"shaoxiong.lin":1709458168,"siyi.zhao":1709458168,"tianteng.gu":1709458168,"tingxiao.zhou":1709458168,"wangyou.zhang":1709458168,"wei.wang":1709458168,"wen.huang":1709458168,"xin.zhou":1709458168,"xun.gong":1709458168,"yanmin.qian":1709458168,"yingpeng.zhang":1709458168,"zhengyang.chen":1709458168},"checksum":"d7eef344097e76475f42e804f8f74d0e"},"04.members\/bing.han":{"key":"members\/bing.han","storage_key":"04.members\/bing.han","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"db52ce4e967b3586761df0cd85b7422f"},"04.members\/chenda.li":{"key":"members\/chenda.li","storage_key":"04.members\/chenda.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"f4abd3ea74eaa7e7e92ef325894aea9e"},"04.members\/chenyang.le":{"key":"members\/chenyang.le","storage_key":"04.members\/chenyang.le","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"91bc6b6c07f7bc0486086181a13a7577"},"04.members\/dongning.yang":{"key":"members\/dongning.yang","storage_key":"04.members\/dongning.yang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"da3b4770ff947fd7e40b105deecc8398"},"04.members\/haibin.yu":{"key":"members\/haibin.yu","storage_key":"04.members\/haibin.yu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"f6f84c239c53727314b885eb0f00b42a"},"04.members\/hang.shao":{"key":"members\/hang.shao","storage_key":"04.members\/hang.shao","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"502ca7c2efa6340b73e32f95a8963cd1"},"04.members\/haoxiang.hou":{"key":"members\/haoxiang.hou","storage_key":"04.members\/haoxiang.hou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"7633478346b4caec38734fab16abfe8e"},"04.members\/haoyu.wang":{"key":"members\/haoyu.wang","storage_key":"04.members\/haoyu.wang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"83c16c5e415b2f2be97ab8fa57efe9f3"},"04.members\/jiahong.li":{"key":"members\/jiahong.li","storage_key":"04.members\/jiahong.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"77255fe055c7ebe6f59e7447a4c620fb"},"04.members\/jianze.li":{"key":"members\/jianze.li","storage_key":"04.members\/jianze.li","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"c80394027a19f75e52ccf7a766446d38"},"04.members\/leying.zhang":{"key":"members\/leying.zhang","storage_key":"04.members\/leying.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"dbd5c9c91ee1bfb9a833dc0c9d284470"},"04.members\/linfeng.yu":{"key":"members\/linfeng.yu","storage_key":"04.members\/linfeng.yu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"dd16f081c1d7bdff4efad1d378e28b0d"},"04.members\/shaoxiong.lin":{"key":"members\/shaoxiong.lin","storage_key":"04.members\/shaoxiong.lin","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"3cf4be20f39575883b5a81df82bab99e"},"04.members\/siyi.zhao":{"key":"members\/siyi.zhao","storage_key":"04.members\/siyi.zhao","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"9ceb84cc3d7f3b074bf0067d3059162b"},"04.members\/tianteng.gu":{"key":"members\/tianteng.gu","storage_key":"04.members\/tianteng.gu","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"67187bf7a1193fd8aee30e1fc86c8f43"},"04.members\/tingxiao.zhou":{"key":"members\/tingxiao.zhou","storage_key":"04.members\/tingxiao.zhou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"ed63f2f8014780fa0f5ed205c46cea6c"},"04.members\/wangyou.zhang":{"key":"members\/wangyou.zhang","storage_key":"04.members\/wangyou.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"51a28fd58681eda2d0594078902427c3"},"04.members\/wei.wang":{"key":"members\/wei.wang","storage_key":"04.members\/wei.wang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"aa7ca181b6dcddd55cf1fcf6c1d1f4a1"},"04.members\/wen.huang":{"key":"members\/wen.huang","storage_key":"04.members\/wen.huang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"51d168e38a1bd962f19024ddba738e74"},"04.members\/xin.zhou":{"key":"members\/xin.zhou","storage_key":"04.members\/xin.zhou","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"528b74a72206a98001ccb374671ee912"},"04.members\/xun.gong":{"key":"members\/xun.gong","storage_key":"04.members\/xun.gong","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"fca850ca2792e41ca340a4dac4952cb1"},"04.members\/yanmin.qian":{"key":"members\/yanmin.qian","storage_key":"04.members\/yanmin.qian","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"333614fff58bd00a07b9ba16d0686829"},"04.members\/yingpeng.zhang":{"key":"members\/yingpeng.zhang","storage_key":"04.members\/yingpeng.zhang","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"9f749cd3fae0a4b264e22bb394cdaca3"},"04.members\/zhengyang.chen":{"key":"members\/zhengyang.chen","storage_key":"04.members\/zhengyang.chen","template":"member","storage_timestamp":1709458168,"markdown":{"en":{"member":1709458168},"zh-hans":{"member":1709458168}},"checksum":"fb627e5575f76faf5df1c4615867e2a4"},"05.contact":{"key":"contact","storage_key":"05.contact","template":"contact","storage_timestamp":1704688085,"markdown":{"en":{"contact":1704590309},"zh-hans":{"contact":1704688085}},"checksum":"f62a4f7ed19c66d126513d81aaad4dd4"},"backup":{"key":"backup","storage_key":"backup","template":null,"storage_timestamp":1704688029,"children":{"04.shortcodes":1704590309,"forgot_password":1704590309,"login":1704590309},"checksum":"9545b23e609082764f510dc35f6923b9"},"backup\/04.shortcodes":{"key":"backup\/shortcodes","storage_key":"backup\/04.shortcodes","template":"default","storage_timestamp":1704590309,"markdown":{"":{"default":1704590309}},"checksum":"6d47876968d7a2645ba335a50eca4161"},"backup\/forgot_password":{"key":"backup\/forgot_password","storage_key":"backup\/forgot_password","template":"forgot","storage_timestamp":1704590309,"markdown":{"":{"forgot":1704590309}},"checksum":"cf6acf1c20cb51798ca76f3c1704714e"},"backup\/login":{"key":"backup\/login","storage_key":"backup\/login","template":"login","storage_timestamp":1704590309,"markdown":{"":{"login":1704590309}},"checksum":"d7968627088617f2cf775aad2d00885c"}}}